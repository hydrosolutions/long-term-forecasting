{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72d6547c",
   "metadata": {},
   "source": [
    "# EDA Seasonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f1e390",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from typing import Dict, Any, Optional\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pingouin as pg\n",
    "\n",
    "# Add the parent directory to sys.path to allow imports from monthly_forecasting\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), os.pardir)))\n",
    "\n",
    "data_config = {\n",
    "    \"path_discharge\": \"../../../../data/discharge/digitized_kyrgyz_hydromet/kyrgyz_hydromet_discharge_daily_2000_2023_kgz_filtered_v2.csv\",\n",
    "    \"path_forcing\": \"../../../../data/forcing/ERA5_krg/HRU00003_forcing_2000-2023.csv\",\n",
    "    \"path_static_data\": \"../../../../GIS/ML_Sandro/ML_basin_attributes_v2.csv\",\n",
    "    \"path_to_sla\": \"../../../../data/sla_silvan/fsc_sla_timeseries_gapfilled.csv\",\n",
    "    \"path_to_nir\": \"../../../../data/sla_silvan/meanNIR_TS_allBasins.csv\",\n",
    "    \"path_to_sca\": None,\n",
    "    \"path_to_hru_shp\": \"../../../../GIS/ML_Sandro/HRU_KRG_ML_MODEL_BASINS/HRU_KRG_ML_MODEL_BASINS_1d.shp\",\n",
    "    \"path_to_swe\": \"../../../../data/snow/kyrgyzstan_ts/SWE\",\n",
    "    \"path_to_hs\": \"../../../../data/snow/kyrgyzstan_ts/HS\",\n",
    "    \"path_to_rof\": \"../../../../data/snow/kyrgyzstan_ts/RoF\",\n",
    "    \"HRU_SWE\": \"HRU_00003\",\n",
    "    \"HRU_HS\": \"HRU_00003\",\n",
    "    \"HRU_ROF\": \"HRU_00003\",\n",
    "    \"model_home_path\": \"../../monthly_forecasting_models/SnowMapper_Based\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eea2cd9",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0307bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from monthly_forecasting.scr import data_loading as dl\n",
    "\n",
    "# supress logging from matplotlib\n",
    "import logging\n",
    "\n",
    "logging.getLogger(\"matplotlib\").setLevel(logging.WARNING)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def load_data(data_config: Dict[str, Any], path_config: Dict[str, Any]) -> tuple:\n",
    "    \"\"\"\n",
    "    Load data using the data loading utilities.\n",
    "\n",
    "    Args:\n",
    "        data_config: Data configuration\n",
    "        path_config: Path configuration\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (data, static_data)\n",
    "    \"\"\"\n",
    "    # -------------- 1. Load Data ------------------------------\n",
    "    hydro_ca, static_df = dl.load_data(\n",
    "        path_discharge=path_config[\"path_discharge\"],\n",
    "        path_forcing=path_config[\"path_forcing\"],\n",
    "        path_static_data=path_config[\"path_static_data\"],\n",
    "        path_to_sca=path_config[\"path_to_sca\"],\n",
    "        path_to_swe=path_config[\"path_to_swe\"],\n",
    "        path_to_hs=path_config[\"path_to_hs\"],\n",
    "        path_to_rof=path_config[\"path_to_rof\"],\n",
    "        HRU_SWE=path_config[\"HRU_SWE\"],\n",
    "        HRU_HS=path_config[\"HRU_HS\"],\n",
    "        HRU_ROF=path_config[\"HRU_ROF\"],\n",
    "        path_to_sla=path_config.get(\"path_to_sla\", None),\n",
    "        path_to_nir=path_config.get(\"path_to_nir\", None),\n",
    "    )\n",
    "\n",
    "    # if log_discharge in columns - drop\n",
    "    if \"log_discharge\" in hydro_ca.columns:\n",
    "        hydro_ca.drop(columns=[\"log_discharge\"], inplace=True)\n",
    "\n",
    "    hydro_ca = hydro_ca.sort_values(\"date\")\n",
    "\n",
    "    hydro_ca[\"code\"] = hydro_ca[\"code\"].astype(int)\n",
    "\n",
    "    if \"CODE\" in static_df.columns:\n",
    "        static_df.rename(columns={\"CODE\": \"code\"}, inplace=True)\n",
    "    static_df[\"code\"] = static_df[\"code\"].astype(int)\n",
    "\n",
    "    return hydro_ca, static_df\n",
    "\n",
    "\n",
    "# Load the data\n",
    "hydro_ca, static_df = load_data(data_config, data_config)\n",
    "\n",
    "print(\"Data loaded successfully.\")\n",
    "\n",
    "# get the dimensions of the data\n",
    "print(f\"hydro_ca shape: {hydro_ca.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c3e72e",
   "metadata": {},
   "source": [
    "## Feature Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f91fe7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_config = {\n",
    "    \"discharge\": [\n",
    "        {\n",
    "            \"operation\": \"mean\",\n",
    "            \"windows\": [\n",
    "                30,\n",
    "            ],\n",
    "            \"lags\": {},\n",
    "        },\n",
    "    ],\n",
    "    \"P\": [{\"operation\": \"sum\", \"windows\": [90], \"lags\": {}}],\n",
    "    \"T\": [{\"operation\": \"mean\", \"windows\": [30], \"lags\": {}}],\n",
    "    \"fsc_basin\": [{\"operation\": \"last_value\", \"windows\": [10, 30], \"lags\": {}}],\n",
    "    \"SWE\": [\n",
    "        {\n",
    "            \"operation\": \"mean\",\n",
    "            \"windows\": [\n",
    "                10,\n",
    "            ],\n",
    "            \"lags\": {},\n",
    "        },\n",
    "        {\"operation\": \"max\", \"windows\": [90], \"lags\": {}},\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f0fca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from monthly_forecasting.scr import FeatureExtractor as FE\n",
    "\n",
    "# Use FeatureExtractor for time series features\n",
    "extractor = FE.StreamflowFeatureExtractor(\n",
    "    feature_configs=feature_config,\n",
    "    prediction_horizon=180,\n",
    "    offset=180,\n",
    ")\n",
    "\n",
    "data = extractor.create_all_features(hydro_ca)\n",
    "\n",
    "# only keep the last day of the month\n",
    "data = data[data[\"date\"].dt.is_month_end]\n",
    "\n",
    "data[\"month\"] = data[\"date\"].dt.month"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2240e48",
   "metadata": {},
   "source": [
    "## Seasonal Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995f8988",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_corr = [\"target\", \"SWE_roll_mean_10\", \"fsc_basin_roll_last_value_10\"]\n",
    "data_march = data[data[\"month\"] == 3].copy()\n",
    "unique_codes = data_march[\"code\"].unique()\n",
    "\n",
    "\n",
    "all_correlations = pd.DataFrame()\n",
    "for code in unique_codes:\n",
    "    data_code = data_march[data_march[\"code\"] == code]\n",
    "    # filter out nan values\n",
    "    data_code = data_code[features_corr].dropna()\n",
    "    if data_code.shape[0] < 4:\n",
    "        continue\n",
    "    corr_month = data_code.corr()\n",
    "    corr_month[\"code\"] = code\n",
    "    all_correlations = pd.concat([all_correlations, corr_month], axis=0)\n",
    "\n",
    "# Plot correlation distribution for swe and fsc\n",
    "fig, axes = plt.subplots(2, 1, figsize=(8, 10))\n",
    "sns.histplot(\n",
    "    all_correlations[\"target\"][\"SWE_roll_mean_10\"], bins=30, kde=True, ax=axes[0]\n",
    ")\n",
    "axes[0].set_title(\"Correlation between Target and SWE_roll_mean_10\", fontsize=14)\n",
    "axes[0].set_xlabel(\"Correlation Coefficient\", fontsize=12)\n",
    "axes[0].set_ylabel(\"Frequency\", fontsize=12)\n",
    "\n",
    "sns.histplot(\n",
    "    all_correlations[\"target\"][\"fsc_basin_roll_last_value_10\"],\n",
    "    bins=30,\n",
    "    kde=True,\n",
    "    ax=axes[1],\n",
    ")\n",
    "axes[1].set_title(\n",
    "    \"Correlation between Target and fsc_basin_roll_last_value_    10\", fontsize=14\n",
    ")\n",
    "axes[1].set_xlabel(\"Correlation Coefficient\", fontsize=12)\n",
    "axes[1].set_ylabel(\"Frequency\", fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# print all basins where correlation between target and swe is less than 0.2\n",
    "for code in unique_codes:\n",
    "    if code not in all_correlations[\"code\"].values:\n",
    "        continue\n",
    "    corr_value = all_correlations[all_correlations[\"code\"] == code][\"target\"][\n",
    "        \"SWE_roll_mean_10\"\n",
    "    ]\n",
    "    if corr_value < 0.2:\n",
    "        print(\n",
    "            f\"Code: {code}, Correlation between Target and SWE_roll_mean_10: {corr_value}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70f1943",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0db37ffe",
   "metadata": {},
   "source": [
    "## LOOCV Linear Regression Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa00343",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Prepare data for March\n",
    "data_march_clean = data_march[\n",
    "    [\"code\", \"target\", \"SWE_roll_mean_10\", \"fsc_basin_roll_last_value_10\"]\n",
    "].dropna()\n",
    "\n",
    "# Initialize results storage\n",
    "loocv_results = []\n",
    "\n",
    "# Iterate through each basin (code)\n",
    "for code in unique_codes:\n",
    "    data_code = data_march_clean[data_march_clean[\"code\"] == code].copy()\n",
    "\n",
    "    # Skip if insufficient data\n",
    "    if data_code.shape[0] < 4:\n",
    "        continue\n",
    "\n",
    "    # Prepare features and target\n",
    "    X_swe = data_code[[\"SWE_roll_mean_10\"]].values\n",
    "    X_fsc = data_code[[\"fsc_basin_roll_last_value_10\"]].values\n",
    "    y = data_code[\"target\"].values\n",
    "\n",
    "    # LOOCV for SWE\n",
    "    loo = LeaveOneOut()\n",
    "    y_pred_swe = np.zeros(len(y))\n",
    "\n",
    "    for train_idx, test_idx in loo.split(X_swe):\n",
    "        X_train, X_test = X_swe[train_idx], X_swe[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        # Fit linear regression\n",
    "        lr_swe = LinearRegression()\n",
    "        lr_swe.fit(X_train, y_train)\n",
    "\n",
    "        # Predict\n",
    "        y_pred_swe[test_idx] = lr_swe.predict(X_test)\n",
    "\n",
    "    # Calculate R² for SWE\n",
    "    r2_swe = r2_score(y, y_pred_swe)\n",
    "\n",
    "    # LOOCV for FSC\n",
    "    y_pred_fsc = np.zeros(len(y))\n",
    "\n",
    "    for train_idx, test_idx in loo.split(X_fsc):\n",
    "        X_train, X_test = X_fsc[train_idx], X_fsc[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        # Fit linear regression\n",
    "        lr_fsc = LinearRegression()\n",
    "        lr_fsc.fit(X_train, y_train)\n",
    "\n",
    "        # Predict\n",
    "        y_pred_fsc[test_idx] = lr_fsc.predict(X_test)\n",
    "\n",
    "    # Calculate R² for FSC\n",
    "    r2_fsc = r2_score(y, y_pred_fsc)\n",
    "\n",
    "    # Store results\n",
    "    loocv_results.append(\n",
    "        {\"code\": code, \"r2_swe\": r2_swe, \"r2_fsc\": r2_fsc, \"n_samples\": len(y)}\n",
    "    )\n",
    "\n",
    "# Create DataFrame with results\n",
    "loocv_df = pd.DataFrame(loocv_results)\n",
    "\n",
    "# Clip R² values to [-1, 1]\n",
    "loocv_df[\"r2_swe_clipped\"] = loocv_df[\"r2_swe\"].clip(-1, 1)\n",
    "loocv_df[\"r2_fsc_clipped\"] = loocv_df[\"r2_fsc\"].clip(-1, 1)\n",
    "\n",
    "print(f\"Number of basins analyzed: {len(loocv_df)}\")\n",
    "print(f\"\\nSWE R² Statistics:\")\n",
    "print(f\"  Mean: {loocv_df['r2_swe_clipped'].mean():.3f}\")\n",
    "print(f\"  Median: {loocv_df['r2_swe_clipped'].median():.3f}\")\n",
    "print(f\"  Std: {loocv_df['r2_swe_clipped'].std():.3f}\")\n",
    "print(f\"\\nFSC R² Statistics:\")\n",
    "print(f\"  Mean: {loocv_df['r2_fsc_clipped'].mean():.3f}\")\n",
    "print(f\"  Median: {loocv_df['r2_fsc_clipped'].median():.3f}\")\n",
    "print(f\"  Std: {loocv_df['r2_fsc_clipped'].std():.3f}\")\n",
    "\n",
    "loocv_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e52c664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histograms of clipped R² values\n",
    "fig, axes = plt.subplots(2, 1, figsize=(10, 10))\n",
    "\n",
    "# Histogram for SWE R²\n",
    "axes[0].hist(\n",
    "    loocv_df[\"r2_swe_clipped\"], bins=30, alpha=0.7, color=\"steelblue\", edgecolor=\"black\"\n",
    ")\n",
    "axes[0].axvline(\n",
    "    loocv_df[\"r2_swe_clipped\"].mean(),\n",
    "    color=\"red\",\n",
    "    linestyle=\"--\",\n",
    "    linewidth=2,\n",
    "    label=f\"Mean: {loocv_df['r2_swe_clipped'].mean():.3f}\",\n",
    ")\n",
    "axes[0].axvline(\n",
    "    loocv_df[\"r2_swe_clipped\"].median(),\n",
    "    color=\"orange\",\n",
    "    linestyle=\"--\",\n",
    "    linewidth=2,\n",
    "    label=f\"Median: {loocv_df['r2_swe_clipped'].median():.3f}\",\n",
    ")\n",
    "axes[0].set_title(\n",
    "    \"LOOCV R² Distribution: SWE Feature (March)\", fontsize=14, fontweight=\"bold\"\n",
    ")\n",
    "axes[0].set_xlabel(\"R² Score (clipped to [-1, 1])\", fontsize=12)\n",
    "axes[0].set_ylabel(\"Frequency\", fontsize=12)\n",
    "axes[0].set_xlim(-1, 1)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].legend(fontsize=10)\n",
    "\n",
    "# Histogram for FSC R²\n",
    "axes[1].hist(\n",
    "    loocv_df[\"r2_fsc_clipped\"],\n",
    "    bins=30,\n",
    "    alpha=0.7,\n",
    "    color=\"forestgreen\",\n",
    "    edgecolor=\"black\",\n",
    ")\n",
    "axes[1].axvline(\n",
    "    loocv_df[\"r2_fsc_clipped\"].mean(),\n",
    "    color=\"red\",\n",
    "    linestyle=\"--\",\n",
    "    linewidth=2,\n",
    "    label=f\"Mean: {loocv_df['r2_fsc_clipped'].mean():.3f}\",\n",
    ")\n",
    "axes[1].axvline(\n",
    "    loocv_df[\"r2_fsc_clipped\"].median(),\n",
    "    color=\"orange\",\n",
    "    linestyle=\"--\",\n",
    "    linewidth=2,\n",
    "    label=f\"Median: {loocv_df['r2_fsc_clipped'].median():.3f}\",\n",
    ")\n",
    "axes[1].set_title(\n",
    "    \"LOOCV R² Distribution: FSC Feature (March)\", fontsize=14, fontweight=\"bold\"\n",
    ")\n",
    "axes[1].set_xlabel(\"R² Score (clipped to [-1, 1])\", fontsize=12)\n",
    "axes[1].set_ylabel(\"Frequency\", fontsize=12)\n",
    "axes[1].set_xlim(-1, 1)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].legend(fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65562d83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6873b661",
   "metadata": {},
   "source": [
    "## Time Series Analysis for Basins with Negative SWE Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a866329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify basins with negative correlation between target and SWE for March\n",
    "negative_swe_basins = []\n",
    "\n",
    "for code in unique_codes:\n",
    "    if code not in all_correlations[\"code\"].values:\n",
    "        continue\n",
    "\n",
    "    # Get correlation value for this basin\n",
    "    corr_data = all_correlations[all_correlations[\"code\"] == code]\n",
    "    if len(corr_data) == 0:\n",
    "        continue\n",
    "\n",
    "    # Extract correlation between target and SWE\n",
    "    try:\n",
    "        corr_value = corr_data.iloc[0][\"SWE_roll_mean_10\"]  # target row, SWE column\n",
    "        if corr_value < 0:  # Negative correlation\n",
    "            negative_swe_basins.append({\"code\": code, \"correlation\": corr_value})\n",
    "    except (KeyError, IndexError):\n",
    "        continue\n",
    "\n",
    "negative_swe_df = pd.DataFrame(negative_swe_basins)\n",
    "print(\n",
    "    f\"Found {len(negative_swe_basins)} basins with negative SWE correlation in March:\"\n",
    ")\n",
    "print(negative_swe_df.sort_values(\"correlation\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c69db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot original time series for basins with negative SWE correlation\n",
    "if len(negative_swe_basins) > 0:\n",
    "    # Calculate subplot grid dimensions\n",
    "    n_basins = len(negative_swe_basins)\n",
    "    n_cols = 1  # Number of columns\n",
    "    n_rows = (n_basins + n_cols - 1) // n_cols  # Ceiling division\n",
    "\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5 * n_rows))\n",
    "\n",
    "    # Ensure axes is always a 2D array for consistent indexing\n",
    "    if n_rows == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    if n_cols == 1:\n",
    "        axes = axes.reshape(-1, 1)\n",
    "\n",
    "    for i, basin_info in enumerate(negative_swe_basins):\n",
    "        code = basin_info[\"code\"]\n",
    "        correlation = basin_info[\"correlation\"]\n",
    "\n",
    "        # Get data for this basin (all months, not just March)\n",
    "        basin_data = hydro_ca[hydro_ca[\"code\"] == code].copy()\n",
    "\n",
    "        if len(basin_data) == 0:\n",
    "            continue\n",
    "\n",
    "        # Sort by date for proper time series plotting\n",
    "        basin_data = basin_data.sort_values(\"date\")\n",
    "\n",
    "        # Calculate subplot position\n",
    "        row = i // n_cols\n",
    "        col = i % n_cols\n",
    "\n",
    "        # Create the plot\n",
    "        ax = axes[row, col]\n",
    "\n",
    "        # Plot discharge (target) and SWE on different y-axes\n",
    "        ax2 = ax.twinx()\n",
    "\n",
    "        # Plot discharge on left y-axis\n",
    "        line1 = ax.plot(\n",
    "            basin_data[\"date\"],\n",
    "            basin_data[\"discharge\"],\n",
    "            color=\"steelblue\",\n",
    "            linewidth=1.5,\n",
    "            label=\"Discharge\",\n",
    "            alpha=0.8,\n",
    "        )\n",
    "\n",
    "        # Plot SWE on right y-axis (if SWE exists)\n",
    "        if \"SWE\" in basin_data.columns:\n",
    "            line2 = ax2.plot(\n",
    "                basin_data[\"date\"],\n",
    "                basin_data[\"SWE\"],\n",
    "                color=\"orange\",\n",
    "                linewidth=1.5,\n",
    "                label=\"SWE\",\n",
    "                alpha=0.8,\n",
    "            )\n",
    "\n",
    "        # Formatting\n",
    "        ax.set_title(\n",
    "            f\"Basin {code}\\nCorr = {correlation:.3f}\", fontsize=12, fontweight=\"bold\"\n",
    "        )\n",
    "        ax.set_xlabel(\"Date\", fontsize=10)\n",
    "        ax.set_ylabel(\"Discharge\", fontsize=10, color=\"steelblue\")\n",
    "        ax2.set_ylabel(\"SWE\", fontsize=10, color=\"orange\")\n",
    "\n",
    "        # Color the y-axis labels to match the lines\n",
    "        ax.tick_params(axis=\"y\", labelcolor=\"steelblue\")\n",
    "        ax2.tick_params(axis=\"y\", labelcolor=\"orange\")\n",
    "\n",
    "        # Add grid\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "        # Rotate x-axis labels for better readability\n",
    "        ax.tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "    # Hide unused subplots\n",
    "    for i in range(len(negative_swe_basins), n_rows * n_cols):\n",
    "        row = i // n_cols\n",
    "        col = i % n_cols\n",
    "        axes[row, col].set_visible(False)\n",
    "\n",
    "    plt.suptitle(\n",
    "        \"Time Series for Basins with Negative SWE Correlation (March)\",\n",
    "        fontsize=16,\n",
    "        fontweight=\"bold\",\n",
    "        y=0.98,\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.95)  # Make room for the main title\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"No basins found with negative SWE correlation in March.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccb86cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b54945bb",
   "metadata": {},
   "source": [
    "## Multiple Linear Regression with Different Predictor Combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b3da56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define predictor combinations for SWE and FSC models\n",
    "swe_combinations = {\n",
    "    \"SWE_T_P_D\": [\n",
    "        \"SWE_roll_mean_10\",\n",
    "        \"T_roll_mean_30\",\n",
    "        \"P_roll_sum_90\",\n",
    "        \"discharge_roll_mean_30\",\n",
    "    ],\n",
    "    \"SWE_P\": [\"SWE_roll_mean_10\", \"P_roll_sum_90\"],\n",
    "    \"SWE_D\": [\"SWE_roll_mean_10\", \"discharge_roll_mean_30\"],\n",
    "    \"T_P_D\": [\"T_roll_mean_30\", \"P_roll_sum_90\", \"discharge_roll_mean_30\"],\n",
    "    \"SWE\": [\"SWE_roll_mean_10\"],\n",
    "}\n",
    "\n",
    "fsc_combinations = {\n",
    "    \"FSC_T_P_D\": [\n",
    "        \"fsc_basin_roll_last_value_10\",\n",
    "        \"T_roll_mean_30\",\n",
    "        \"P_roll_sum_90\",\n",
    "        \"discharge_roll_mean_30\",\n",
    "    ],\n",
    "    \"FSC_P\": [\"fsc_basin_roll_last_value_10\", \"P_roll_sum_90\"],\n",
    "    \"FSC_D\": [\"fsc_basin_roll_last_value_10\", \"discharge_roll_mean_30\"],\n",
    "    \"T_P_D\": [\n",
    "        \"T_roll_mean_30\",\n",
    "        \"P_roll_sum_90\",\n",
    "        \"discharge_roll_mean_30\",\n",
    "    ],  # Same as above\n",
    "    \"FSC\": [\"fsc_basin_roll_last_value_10\"],\n",
    "}\n",
    "\n",
    "# Prepare data for March with all required features\n",
    "required_features = [\n",
    "    \"target\",\n",
    "    \"SWE_roll_mean_10\",\n",
    "    \"fsc_basin_roll_last_value_10\",\n",
    "    \"T_roll_mean_30\",\n",
    "    \"P_roll_sum_90\",\n",
    "    \"discharge_roll_mean_30\",\n",
    "]\n",
    "\n",
    "# Check which features are available in the data\n",
    "available_features = [col for col in required_features if col in data_march.columns]\n",
    "print(f\"Available features: {available_features}\")\n",
    "\n",
    "# Use only available features for the analysis\n",
    "data_march_reg = data_march[[\"code\"] + available_features].dropna()\n",
    "print(f\"Data shape after removing NaN: {data_march_reg.shape}\")\n",
    "print(f\"Available basins: {data_march_reg['code'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a50522f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fit regression models and calculate R²\n",
    "def fit_regression_models(data, combinations, combination_type=\"SWE\"):\n",
    "    \"\"\"\n",
    "    Fit linear regression models for different predictor combinations.\n",
    "\n",
    "    Args:\n",
    "        data: DataFrame with features and target\n",
    "        combinations: Dict with combination names and feature lists\n",
    "        combination_type: String identifier for the type of combinations\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with R² results for each basin and combination\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    unique_codes = data[\"code\"].unique()\n",
    "\n",
    "    for code in unique_codes:\n",
    "        basin_data = data[data[\"code\"] == code].copy()\n",
    "\n",
    "        # Skip if insufficient data\n",
    "        if basin_data.shape[0] < 5:\n",
    "            continue\n",
    "\n",
    "        y = basin_data[\"target\"].values\n",
    "\n",
    "        # Fit models for each combination\n",
    "        for combo_name, features in combinations.items():\n",
    "            # Check if all features are available\n",
    "            available_features = [f for f in features if f in basin_data.columns]\n",
    "\n",
    "            if len(available_features) == 0:\n",
    "                continue\n",
    "\n",
    "            # Use only available features\n",
    "            X = basin_data[available_features].values\n",
    "\n",
    "            # Check for NaN values\n",
    "            if np.isnan(X).any() or np.isnan(y).any():\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                # Fit linear regression\n",
    "                lr = LinearRegression()\n",
    "                lr.fit(X, y)\n",
    "\n",
    "                # Calculate R²\n",
    "                y_pred = lr.predict(X)\n",
    "                r2 = r2_score(y, y_pred)\n",
    "\n",
    "                # Store results\n",
    "                results.append(\n",
    "                    {\n",
    "                        \"code\": code,\n",
    "                        \"combination\": combo_name,\n",
    "                        \"r2\": r2,\n",
    "                        \"r2_clipped\": np.clip(r2, -1, 1),\n",
    "                        \"n_samples\": len(y),\n",
    "                        \"n_features\": len(available_features),\n",
    "                        \"features_used\": \", \".join(available_features),\n",
    "                        \"type\": combination_type,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            except Exception as e:\n",
    "                print(\n",
    "                    f\"Error fitting model for basin {code}, combination {combo_name}: {e}\"\n",
    "                )\n",
    "                continue\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "# Fit SWE-based models\n",
    "print(\"Fitting SWE-based regression models...\")\n",
    "swe_results = fit_regression_models(data_march_reg, swe_combinations, \"SWE\")\n",
    "\n",
    "# Fit FSC-based models\n",
    "print(\"Fitting FSC-based regression models...\")\n",
    "fsc_results = fit_regression_models(data_march_reg, fsc_combinations, \"FSC\")\n",
    "\n",
    "# Combine results\n",
    "all_results = pd.concat([swe_results, fsc_results], ignore_index=True)\n",
    "\n",
    "print(f\"\\nTotal model fits: {len(all_results)}\")\n",
    "print(f\"SWE-based models: {len(swe_results)}\")\n",
    "print(f\"FSC-based models: {len(fsc_results)}\")\n",
    "\n",
    "# Display summary statistics\n",
    "print(f\"\\nR² Statistics by Combination:\")\n",
    "summary_stats = (\n",
    "    all_results.groupby([\"type\", \"combination\"])[\"r2_clipped\"]\n",
    "    .agg([\"count\", \"mean\", \"median\", \"std\", \"min\", \"max\"])\n",
    "    .round(3)\n",
    ")\n",
    "print(summary_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f98c26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create histogram plots for SWE-based combinations\n",
    "swe_combinations_order = [\"SWE_T_P_D\", \"SWE_D\", \"T_P_D\", \"SWE\"]\n",
    "swe_data = swe_results[swe_results[\"combination\"].isin(swe_combinations_order)]\n",
    "\n",
    "if len(swe_data) > 0:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    colors = [\"steelblue\", \"forestgreen\", \"coral\", \"purple\"]\n",
    "\n",
    "    for i, combo in enumerate(swe_combinations_order):\n",
    "        combo_data = swe_data[swe_data[\"combination\"] == combo][\"r2_clipped\"]\n",
    "\n",
    "        if len(combo_data) > 0:\n",
    "            ax = axes[i]\n",
    "\n",
    "            # Create histogram\n",
    "            ax.hist(combo_data, bins=20, alpha=0.7, color=colors[i], edgecolor=\"black\")\n",
    "\n",
    "            # Add statistics lines\n",
    "            mean_val = combo_data.mean()\n",
    "            median_val = combo_data.median()\n",
    "\n",
    "            ax.axvline(\n",
    "                mean_val,\n",
    "                color=\"red\",\n",
    "                linestyle=\"--\",\n",
    "                linewidth=2,\n",
    "                label=f\"Mean: {mean_val:.3f}\",\n",
    "            )\n",
    "            ax.axvline(\n",
    "                median_val,\n",
    "                color=\"orange\",\n",
    "                linestyle=\"--\",\n",
    "                linewidth=2,\n",
    "                label=f\"Median: {median_val:.3f}\",\n",
    "            )\n",
    "\n",
    "            # Formatting\n",
    "            ax.set_title(\n",
    "                f\"{combo} (n={len(combo_data)})\", fontsize=12, fontweight=\"bold\"\n",
    "            )\n",
    "            ax.set_xlabel(\"R² Score (clipped to [-1, 1])\", fontsize=10)\n",
    "            ax.set_ylabel(\"Frequency\", fontsize=10)\n",
    "            ax.set_xlim(-1, 1)\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            ax.legend(fontsize=9)\n",
    "\n",
    "            # Add text box with additional stats\n",
    "            stats_text = f\"Std: {combo_data.std():.3f}\\nMin: {combo_data.min():.3f}\\nMax: {combo_data.max():.3f}\"\n",
    "            ax.text(\n",
    "                0.02,\n",
    "                0.98,\n",
    "                stats_text,\n",
    "                transform=ax.transAxes,\n",
    "                fontsize=8,\n",
    "                verticalalignment=\"top\",\n",
    "                bbox=dict(boxstyle=\"round\", facecolor=\"wheat\", alpha=0.8),\n",
    "            )\n",
    "\n",
    "    plt.suptitle(\n",
    "        \"R² Distribution for SWE-based Regression Models (March)\",\n",
    "        fontsize=16,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No SWE-based results to plot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c485e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create histogram plots for FSC-based combinations\n",
    "fsc_combinations_order = [\"FSC_T_P_D\", \"FSC_P\", \"FSC_D\", \"FSC\"]\n",
    "fsc_data = fsc_results[fsc_results[\"combination\"].isin(fsc_combinations_order)]\n",
    "\n",
    "if len(fsc_data) > 0:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    colors = [\"darkblue\", \"darkgreen\", \"darkorange\", \"darkred\"]\n",
    "\n",
    "    for i, combo in enumerate(fsc_combinations_order):\n",
    "        combo_data = fsc_data[fsc_data[\"combination\"] == combo][\"r2_clipped\"]\n",
    "\n",
    "        if len(combo_data) > 0:\n",
    "            ax = axes[i]\n",
    "\n",
    "            # Create histogram\n",
    "            ax.hist(combo_data, bins=20, alpha=0.7, color=colors[i], edgecolor=\"black\")\n",
    "\n",
    "            # Add statistics lines\n",
    "            mean_val = combo_data.mean()\n",
    "            median_val = combo_data.median()\n",
    "\n",
    "            ax.axvline(\n",
    "                mean_val,\n",
    "                color=\"red\",\n",
    "                linestyle=\"--\",\n",
    "                linewidth=2,\n",
    "                label=f\"Mean: {mean_val:.3f}\",\n",
    "            )\n",
    "            ax.axvline(\n",
    "                median_val,\n",
    "                color=\"orange\",\n",
    "                linestyle=\"--\",\n",
    "                linewidth=2,\n",
    "                label=f\"Median: {median_val:.3f}\",\n",
    "            )\n",
    "\n",
    "            # Formatting\n",
    "            ax.set_title(\n",
    "                f\"{combo} (n={len(combo_data)})\", fontsize=12, fontweight=\"bold\"\n",
    "            )\n",
    "            ax.set_xlabel(\"R² Score (clipped to [-1, 1])\", fontsize=10)\n",
    "            ax.set_ylabel(\"Frequency\", fontsize=10)\n",
    "            ax.set_xlim(-1, 1)\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            ax.legend(fontsize=9)\n",
    "\n",
    "            # Add text box with additional stats\n",
    "            stats_text = f\"Std: {combo_data.std():.3f}\\nMin: {combo_data.min():.3f}\\nMax: {combo_data.max():.3f}\"\n",
    "            ax.text(\n",
    "                0.02,\n",
    "                0.98,\n",
    "                stats_text,\n",
    "                transform=ax.transAxes,\n",
    "                fontsize=8,\n",
    "                verticalalignment=\"top\",\n",
    "                bbox=dict(boxstyle=\"round\", facecolor=\"lightblue\", alpha=0.8),\n",
    "            )\n",
    "\n",
    "    plt.suptitle(\n",
    "        \"R² Distribution for FSC-based Regression Models (March)\",\n",
    "        fontsize=16,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No FSC-based results to plot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b748a083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison plot: SWE vs FSC for similar combinations\n",
    "comparison_pairs = [\n",
    "    (\"SWE_T_P_D\", \"FSC_T_P_D\"),\n",
    "    (\"SWE_P\", \"FSC_P\"),\n",
    "    (\"SWE_D\", \"FSC_D\"),\n",
    "    (\"SWE\", \"FSC\"),\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(18, 6))\n",
    "\n",
    "for i, (swe_combo, fsc_combo) in enumerate(comparison_pairs):\n",
    "    ax = axes[i]\n",
    "\n",
    "    # Get data for both combinations\n",
    "    swe_combo_data = swe_results[swe_results[\"combination\"] == swe_combo][\"r2_clipped\"]\n",
    "    fsc_combo_data = fsc_results[fsc_results[\"combination\"] == fsc_combo][\"r2_clipped\"]\n",
    "\n",
    "    if len(swe_combo_data) > 0 and len(fsc_combo_data) > 0:\n",
    "        # Create overlapping histograms\n",
    "        ax.hist(\n",
    "            swe_combo_data,\n",
    "            bins=15,\n",
    "            alpha=0.6,\n",
    "            label=f\"SWE-based (n={len(swe_combo_data)})\",\n",
    "            color=\"steelblue\",\n",
    "            edgecolor=\"black\",\n",
    "        )\n",
    "        ax.hist(\n",
    "            fsc_combo_data,\n",
    "            bins=15,\n",
    "            alpha=0.6,\n",
    "            label=f\"FSC-based (n={len(fsc_combo_data)})\",\n",
    "            color=\"forestgreen\",\n",
    "            edgecolor=\"black\",\n",
    "        )\n",
    "\n",
    "        # Add mean lines\n",
    "        ax.axvline(\n",
    "            swe_combo_data.mean(),\n",
    "            color=\"blue\",\n",
    "            linestyle=\"--\",\n",
    "            linewidth=2,\n",
    "            label=f\"SWE Mean: {swe_combo_data.mean():.3f}\",\n",
    "        )\n",
    "        ax.axvline(\n",
    "            fsc_combo_data.mean(),\n",
    "            color=\"green\",\n",
    "            linestyle=\"--\",\n",
    "            linewidth=2,\n",
    "            label=f\"FSC Mean: {fsc_combo_data.mean():.3f}\",\n",
    "        )\n",
    "\n",
    "    # Formatting\n",
    "    combo_title = swe_combo.replace(\"SWE_\", \"\").replace(\"FSC_\", \"\")\n",
    "    ax.set_title(f\"{combo_title}\", fontsize=12, fontweight=\"bold\")\n",
    "    ax.set_xlabel(\"R² Score (clipped to [-1, 1])\", fontsize=10)\n",
    "    ax.set_ylabel(\"Frequency\", fontsize=10)\n",
    "    ax.set_xlim(-1, 1)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend(fontsize=8)\n",
    "\n",
    "plt.suptitle(\n",
    "    \"SWE vs FSC Model Performance Comparison (March)\", fontsize=16, fontweight=\"bold\"\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print detailed comparison statistics\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DETAILED COMPARISON STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for swe_combo, fsc_combo in comparison_pairs:\n",
    "    swe_data = swe_results[swe_results[\"combination\"] == swe_combo][\"r2_clipped\"]\n",
    "    fsc_data = fsc_results[fsc_results[\"combination\"] == fsc_combo][\"r2_clipped\"]\n",
    "\n",
    "    if len(swe_data) > 0 and len(fsc_data) > 0:\n",
    "        combo_name = swe_combo.replace(\"SWE_\", \"\").replace(\"_\", \"+\")\n",
    "        print(f\"\\n{combo_name}:\")\n",
    "        print(\n",
    "            f\"  SWE-based: Mean={swe_data.mean():.3f}, Std={swe_data.std():.3f}, n={len(swe_data)}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"  FSC-based: Mean={fsc_data.mean():.3f}, Std={fsc_data.std():.3f}, n={len(fsc_data)}\"\n",
    "        )\n",
    "        print(f\"  Difference (FSC - SWE): {fsc_data.mean() - swe_data.mean():.3f}\")\n",
    "\n",
    "# Save results to CSV for further analysis\n",
    "all_results.to_csv(\"regression_model_results.csv\", index=False)\n",
    "print(f\"\\nResults saved to 'regression_model_results.csv'\")\n",
    "print(f\"Total models fitted: {len(all_results)}\")\n",
    "print(f\"Unique basins: {all_results['code'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64169956",
   "metadata": {},
   "source": [
    "## Hierarchical Linear Regression Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d786686b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install statsmodels if not available\n",
    "try:\n",
    "    import statsmodels.api as sm\n",
    "    from statsmodels.formula.api import mixedlm\n",
    "\n",
    "    print(\"Statsmodels is available\")\n",
    "except ImportError:\n",
    "    raise ImportError(\n",
    "        \"Please install statsmodels package to run mixed-effects modeling.\"\n",
    "    )\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import LeaveOneOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d86048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Preprocess features (z-score normalization per basin)\n",
    "def preprocess_hierarchical_data(data):\n",
    "    \"\"\"\n",
    "    Preprocess data for hierarchical regression with z-score normalization per basin.\n",
    "\n",
    "    Args:\n",
    "        data: DataFrame with features and target\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with normalized features\n",
    "    \"\"\"\n",
    "    # Define features to normalize\n",
    "    features_to_normalize = [\n",
    "        \"SWE_roll_mean_10\",\n",
    "        \"T_roll_mean_30\",\n",
    "        \"P_roll_sum_90\",\n",
    "        \"discharge_roll_mean_30\",\n",
    "    ]\n",
    "\n",
    "    # Check which features are available\n",
    "    available_features = [f for f in features_to_normalize if f in data.columns]\n",
    "    print(f\"Features to normalize: {available_features}\")\n",
    "\n",
    "    # Create a copy of the data\n",
    "    data_processed = data.copy()\n",
    "\n",
    "    # Normalize features per basin (z-score normalization)\n",
    "    for code in data[\"code\"].unique():\n",
    "        basin_mask = data_processed[\"code\"] == code\n",
    "        basin_data = data_processed.loc[basin_mask, available_features]\n",
    "\n",
    "        # Skip if insufficient data\n",
    "        if basin_data.shape[0] < 3:\n",
    "            continue\n",
    "\n",
    "        # Apply z-score normalization\n",
    "        scaler = StandardScaler()\n",
    "        data_processed.loc[basin_mask, available_features] = scaler.fit_transform(\n",
    "            basin_data\n",
    "        )\n",
    "\n",
    "    # Rename columns to indicate they are normalized\n",
    "    rename_dict = {f: f + \"_norm\" for f in available_features}\n",
    "    data_processed = data_processed.rename(columns=rename_dict)\n",
    "\n",
    "    return data_processed, available_features\n",
    "\n",
    "\n",
    "# Prepare data for hierarchical modeling\n",
    "print(\"Preprocessing data for hierarchical regression...\")\n",
    "hierarchical_features = [\n",
    "    \"target\",\n",
    "    \"SWE_roll_mean_10\",\n",
    "    \"T_roll_mean_30\",\n",
    "    \"P_roll_sum_90\",\n",
    "    \"discharge_roll_mean_30\",\n",
    "]\n",
    "available_hier_features = [f for f in hierarchical_features if f in data_march.columns]\n",
    "\n",
    "# Get clean data for hierarchical modeling\n",
    "hier_data = data_march[[\"code\"] + available_hier_features].dropna()\n",
    "print(f\"Data shape before preprocessing: {hier_data.shape}\")\n",
    "\n",
    "# Preprocess the data\n",
    "hier_data_processed, normalized_features = preprocess_hierarchical_data(hier_data)\n",
    "print(f\"Data shape after preprocessing: {hier_data_processed.shape}\")\n",
    "print(f\"Number of basins: {hier_data_processed['code'].nunique()}\")\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nFirst few rows of processed data:\")\n",
    "print(hier_data_processed.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dd4d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define hierarchical regression models with different parameter combinations\n",
    "def fit_hierarchical_model(data, formula, model_name):\n",
    "    \"\"\"\n",
    "    Fit a hierarchical linear regression model using mixed effects.\n",
    "\n",
    "    Args:\n",
    "        data: DataFrame with normalized features\n",
    "        formula: String formula for the mixed effects model\n",
    "        model_name: Name identifier for the model\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with model results\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Fit mixed effects model\n",
    "        model = mixedlm(formula, data, groups=data[\"code\"])\n",
    "        result = model.fit(method=\"lbfgs\")\n",
    "\n",
    "        # Calculate predictions and R²\n",
    "        predictions = result.fittedvalues\n",
    "        r2 = r2_score(data[\"target\"], predictions)\n",
    "\n",
    "        return {\n",
    "            \"model_name\": model_name,\n",
    "            \"formula\": formula,\n",
    "            \"model\": result,\n",
    "            \"predictions\": predictions,\n",
    "            \"r2\": r2,\n",
    "            \"aic\": result.aic,\n",
    "            \"bic\": result.bic,\n",
    "            \"success\": True,\n",
    "            \"error\": None,\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fitting {model_name}: {e}\")\n",
    "        return {\n",
    "            \"model_name\": model_name,\n",
    "            \"formula\": formula,\n",
    "            \"model\": None,\n",
    "            \"predictions\": None,\n",
    "            \"r2\": None,\n",
    "            \"aic\": None,\n",
    "            \"bic\": None,\n",
    "            \"success\": False,\n",
    "            \"error\": str(e),\n",
    "        }\n",
    "\n",
    "\n",
    "# Define different hierarchical model combinations\n",
    "# Format: formula, model_name\n",
    "hierarchical_models = []\n",
    "\n",
    "# Check which normalized features are available\n",
    "norm_features = [f + \"_norm\" for f in normalized_features]\n",
    "available_norm = [f for f in norm_features if f in hier_data_processed.columns]\n",
    "\n",
    "print(f\"Available normalized features: {available_norm}\")\n",
    "\n",
    "# Create model formulas based on available features\n",
    "if (\n",
    "    \"SWE_roll_mean_10_norm\" in available_norm\n",
    "    and \"T_roll_mean_30_norm\" in available_norm\n",
    "    and \"P_roll_sum_90_norm\" in available_norm\n",
    "    and \"discharge_roll_mean_30_norm\" in available_norm\n",
    "):\n",
    "    # Model 4: Discharge fixed globally, SWE/T/P vary by basin\n",
    "    hierarchical_models.append(\n",
    "        (\n",
    "            \"target ~ discharge_roll_mean_30_norm + SWE_roll_mean_10_norm + T_roll_mean_30_norm + P_roll_sum_90_norm\",\n",
    "            \"SWE_regional\",\n",
    "            \"SWE_roll_mean_10_norm\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Model 5: SWE fixed globally, SWE + T vary by basin\n",
    "    hierarchical_models.append(\n",
    "        (\n",
    "            \"target ~ T_roll_mean_30_norm + discharge_roll_mean_30_norm + SWE_roll_mean_10_norm + P_roll_sum_90_norm\",\n",
    "            \"SWE_T_regional\",\n",
    "            \"SWE_roll_mean_10_norm + T_roll_mean_30_norm\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Model 4: Discharge fixed globally, SWE/T/P vary by basin\n",
    "    hierarchical_models.append(\n",
    "        (\n",
    "            \"target ~ discharge_roll_mean_30_norm + SWE_roll_mean_10_norm + T_roll_mean_30_norm + P_roll_sum_90_norm\",\n",
    "            \"SWE_discharge_regional\",\n",
    "            \"SWE_roll_mean_10_norm + discharge_roll_mean_30_norm\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "print(f\"Defined {len(hierarchical_models)} hierarchical models to fit\")\n",
    "\n",
    "# Fit all hierarchical models\n",
    "hierarchical_results = []\n",
    "\n",
    "for fixed_formula, model_name, random_formula in hierarchical_models:\n",
    "    print(f\"\\nFitting model: {model_name}\")\n",
    "    print(f\"Fixed effects: {fixed_formula}\")\n",
    "    print(f\"Random effects: {random_formula}\")\n",
    "\n",
    "    # Construct the full formula for mixedlm\n",
    "    # In statsmodels, we specify random effects using re_formula\n",
    "    try:\n",
    "        model = mixedlm(\n",
    "            fixed_formula,\n",
    "            hier_data_processed,\n",
    "            groups=hier_data_processed[\"code\"],\n",
    "            re_formula=random_formula,\n",
    "        )\n",
    "        result = model.fit(method=\"lbfgs\")\n",
    "\n",
    "        # Calculate predictions and R²\n",
    "        predictions = result.fittedvalues\n",
    "        r2 = r2_score(hier_data_processed[\"target\"], predictions)\n",
    "\n",
    "        hierarchical_results.append(\n",
    "            {\n",
    "                \"model_name\": model_name,\n",
    "                \"fixed_formula\": fixed_formula,\n",
    "                \"random_formula\": random_formula,\n",
    "                \"model\": result,\n",
    "                \"predictions\": predictions,\n",
    "                \"r2\": r2,\n",
    "                \"r2_clipped\": np.clip(r2, -1, 1),\n",
    "                \"aic\": result.aic,\n",
    "                \"bic\": result.bic,\n",
    "                \"success\": True,\n",
    "                \"n_obs\": len(predictions),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        print(f\"  R² = {r2:.4f}\")\n",
    "        print(f\"  AIC = {result.aic:.2f}\")\n",
    "        print(f\"  BIC = {result.bic:.2f}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  Error: {e}\")\n",
    "        hierarchical_results.append(\n",
    "            {\n",
    "                \"model_name\": model_name,\n",
    "                \"fixed_formula\": fixed_formula,\n",
    "                \"random_formula\": random_formula,\n",
    "                \"model\": None,\n",
    "                \"predictions\": None,\n",
    "                \"r2\": None,\n",
    "                \"r2_clipped\": None,\n",
    "                \"aic\": None,\n",
    "                \"bic\": None,\n",
    "                \"success\": False,\n",
    "                \"n_obs\": 0,\n",
    "            }\n",
    "        )\n",
    "\n",
    "# Create summary DataFrame\n",
    "hier_summary = pd.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            \"Model\": result[\"model_name\"],\n",
    "            \"R²\": result[\"r2\"],\n",
    "            \"R²_clipped\": result[\"r2_clipped\"],\n",
    "            \"AIC\": result[\"aic\"],\n",
    "            \"BIC\": result[\"bic\"],\n",
    "            \"Success\": result[\"success\"],\n",
    "            \"N_obs\": result[\"n_obs\"],\n",
    "        }\n",
    "        for result in hierarchical_results\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"HIERARCHICAL MODEL SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(hier_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbf95fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Implement yearly LOOCV for hierarchical models\n",
    "def hierarchical_yearly_loocv(data, fixed_formula, random_formula, model_name):\n",
    "    \"\"\"\n",
    "    Perform yearly Leave-One-Out Cross-Validation for hierarchical models.\n",
    "    Leaves out one year at a time across all basins.\n",
    "\n",
    "    Args:\n",
    "        data: DataFrame with features and target (must have 'date' column)\n",
    "        fixed_formula: Formula for fixed effects\n",
    "        random_formula: Formula for random effects\n",
    "        model_name: Name of the model\n",
    "\n",
    "    Returns:\n",
    "        List of R² scores from yearly LOOCV\n",
    "    \"\"\"\n",
    "    # Extract years from the data\n",
    "    # We'll merge back with the original March data to get dates\n",
    "    data_with_dates = data.merge(\n",
    "        data_march[[\"code\", \"target\", \"date\"]], on=[\"code\", \"target\"], how=\"left\"\n",
    "    )\n",
    "\n",
    "    # Get unique years\n",
    "    data_with_dates[\"year\"] = pd.to_datetime(data_with_dates[\"date\"]).dt.year\n",
    "    unique_years = sorted(data_with_dates[\"year\"].unique())\n",
    "\n",
    "    yearly_loocv_r2_scores = []\n",
    "\n",
    "    print(f\"Performing yearly LOOCV for {model_name}...\")\n",
    "    print(f\"  Available years: {unique_years}\")\n",
    "\n",
    "    for i, year_out in enumerate(unique_years):\n",
    "        print(f\"  Progress: {i + 1}/{len(unique_years)} - Leaving out year {year_out}\")\n",
    "\n",
    "        try:\n",
    "            # Split data: leave one year out across all basins\n",
    "            train_data = data_with_dates[data_with_dates[\"year\"] != year_out].copy()\n",
    "            test_data = data_with_dates[data_with_dates[\"year\"] == year_out].copy()\n",
    "\n",
    "            # Skip if insufficient training or test data\n",
    "            if len(train_data) < 20 or len(test_data) < 5:\n",
    "                print(\n",
    "                    f\"    Insufficient data for year {year_out}: train={len(train_data)}, test={len(test_data)}\"\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            # Remove date and year columns for modeling\n",
    "            train_model_data = train_data.drop(\n",
    "                columns=[\"date\", \"year\"], errors=\"ignore\"\n",
    "            )\n",
    "            test_model_data = test_data.drop(columns=[\"date\", \"year\"], errors=\"ignore\")\n",
    "\n",
    "            # Fit model on training data\n",
    "            model = mixedlm(\n",
    "                fixed_formula,\n",
    "                train_model_data,\n",
    "                groups=train_model_data[\"code\"],\n",
    "                re_formula=random_formula,\n",
    "            )\n",
    "            result = model.fit(method=\"lbfgs\", maxiter=100)\n",
    "\n",
    "            # Make predictions on test data\n",
    "            # For hierarchical models, we can predict using both fixed and random effects\n",
    "            # since the basins in test set were seen during training (just different years)\n",
    "            if hasattr(result, \"fittedvalues\") and len(test_model_data) > 0:\n",
    "                # Get predictions for test data\n",
    "                test_predictions = result.predict(test_model_data)\n",
    "\n",
    "                # Calculate R² for this year\n",
    "                y_true = test_model_data[\"target\"].values\n",
    "                y_pred = test_predictions.values\n",
    "\n",
    "                if len(y_true) > 1:\n",
    "                    r2 = r2_score(y_true, y_pred)\n",
    "                    yearly_loocv_r2_scores.append(np.clip(r2, -1, 1))\n",
    "                    print(\n",
    "                        f\"    Year {year_out}: R² = {r2:.4f} (n_test = {len(y_true)})\"\n",
    "                    )\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"    Error in year {year_out}: {e}\")\n",
    "            continue\n",
    "\n",
    "    return yearly_loocv_r2_scores\n",
    "\n",
    "\n",
    "# Perform LOOCV for successful hierarchical models\n",
    "loocv_hierarchical_results = []\n",
    "\n",
    "for result in hierarchical_results:\n",
    "    if result[\"success\"]:\n",
    "        model_name = result[\"model_name\"]\n",
    "        fixed_formula = result[\"fixed_formula\"]\n",
    "        random_formula = result[\"random_formula\"]\n",
    "\n",
    "        # Perform yearly LOOCV\n",
    "        loocv_scores = hierarchical_yearly_loocv(\n",
    "            hier_data_processed, fixed_formula, random_formula, model_name\n",
    "        )\n",
    "\n",
    "        if len(loocv_scores) > 0:\n",
    "            loocv_hierarchical_results.append(\n",
    "                {\n",
    "                    \"model_name\": model_name,\n",
    "                    \"loocv_r2_scores\": loocv_scores,\n",
    "                    \"loocv_r2_mean\": np.mean(loocv_scores),\n",
    "                    \"loocv_r2_std\": np.std(loocv_scores),\n",
    "                    \"loocv_r2_median\": np.median(loocv_scores),\n",
    "                    \"n_folds\": len(loocv_scores),\n",
    "                    \"full_data_r2\": result[\"r2\"],\n",
    "                }\n",
    "            )\n",
    "\n",
    "            print(f\"\\nLOOCV Results for {model_name}:\")\n",
    "            print(\n",
    "                f\"  Mean R² = {np.mean(loocv_scores):.4f} ± {np.std(loocv_scores):.4f}\"\n",
    "            )\n",
    "            print(f\"  Median R² = {np.median(loocv_scores):.4f}\")\n",
    "            print(f\"  Number of folds = {len(loocv_scores)}\")\n",
    "\n",
    "# Create LOOCV summary DataFrame\n",
    "loocv_hier_summary = pd.DataFrame(loocv_hierarchical_results)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"HIERARCHICAL MODEL LOOCV SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "if len(loocv_hier_summary) > 0:\n",
    "    display_cols = [\n",
    "        \"model_name\",\n",
    "        \"full_data_r2\",\n",
    "        \"loocv_r2_mean\",\n",
    "        \"loocv_r2_std\",\n",
    "        \"n_folds\",\n",
    "    ]\n",
    "    print(loocv_hier_summary[display_cols].round(4))\n",
    "else:\n",
    "    print(\"No successful LOOCV results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39233bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Visualize R² distributions for hierarchical models\n",
    "# Plot full data R² distribution\n",
    "successful_hier_results = [r for r in hierarchical_results if r[\"success\"]]\n",
    "\n",
    "if len(successful_hier_results) > 0:\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(12, 10))\n",
    "\n",
    "    # Full data R² distribution\n",
    "    ax1 = axes[0]\n",
    "    model_names = [r[\"model_name\"] for r in successful_hier_results]\n",
    "    r2_values = [r[\"r2_clipped\"] for r in successful_hier_results]\n",
    "\n",
    "    bars = ax1.bar(\n",
    "        range(len(model_names)),\n",
    "        r2_values,\n",
    "        color=[\"steelblue\", \"forestgreen\", \"coral\", \"purple\", \"orange\"][\n",
    "            : len(model_names)\n",
    "        ],\n",
    "        alpha=0.7,\n",
    "        edgecolor=\"black\",\n",
    "    )\n",
    "\n",
    "    # Add value labels on bars\n",
    "    for i, (bar, r2) in enumerate(zip(bars, r2_values)):\n",
    "        ax1.text(\n",
    "            bar.get_x() + bar.get_width() / 2,\n",
    "            bar.get_height() + 0.01,\n",
    "            f\"{r2:.3f}\",\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\",\n",
    "            fontweight=\"bold\",\n",
    "        )\n",
    "\n",
    "    ax1.set_title(\n",
    "        \"Hierarchical Model Performance - Full Data R²\", fontsize=14, fontweight=\"bold\"\n",
    "    )\n",
    "    ax1.set_xlabel(\"Model Type\", fontsize=12)\n",
    "    ax1.set_ylabel(\"R² Score\", fontsize=12)\n",
    "    ax1.set_xticks(range(len(model_names)))\n",
    "    ax1.set_xticklabels(\n",
    "        [name.replace(\"_\", \"\\n\") for name in model_names], rotation=45, ha=\"right\"\n",
    "    )\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_ylim(0, 1)\n",
    "\n",
    "    # LOOCV R² distributions (box plot)\n",
    "    if len(loocv_hierarchical_results) > 0:\n",
    "        ax2 = axes[1]\n",
    "\n",
    "        loocv_data = []\n",
    "        loocv_labels = []\n",
    "\n",
    "        for result in loocv_hierarchical_results:\n",
    "            loocv_data.append(result[\"loocv_r2_scores\"])\n",
    "            loocv_labels.append(result[\"model_name\"].replace(\"_\", \"\\n\"))\n",
    "\n",
    "        box_plot = ax2.boxplot(loocv_data, labels=loocv_labels, patch_artist=True)\n",
    "\n",
    "        # Color the boxes\n",
    "        colors = [\"steelblue\", \"forestgreen\", \"coral\", \"purple\", \"orange\"]\n",
    "        for patch, color in zip(box_plot[\"boxes\"], colors[: len(box_plot[\"boxes\"])]):\n",
    "            patch.set_facecolor(color)\n",
    "            patch.set_alpha(0.7)\n",
    "\n",
    "        ax2.set_title(\n",
    "            \"Hierarchical Model Performance - LOOCV R² Distribution\",\n",
    "            fontsize=14,\n",
    "            fontweight=\"bold\",\n",
    "        )\n",
    "        ax2.set_xlabel(\"Model Type\", fontsize=12)\n",
    "        ax2.set_ylabel(\"R² Score (LOOCV)\", fontsize=12)\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        ax2.set_ylim(-1, 1)\n",
    "\n",
    "        # Add mean markers\n",
    "        for i, result in enumerate(loocv_hierarchical_results):\n",
    "            mean_val = result[\"loocv_r2_mean\"]\n",
    "            ax2.plot(\n",
    "                i + 1,\n",
    "                mean_val,\n",
    "                \"D\",\n",
    "                color=\"red\",\n",
    "                markersize=8,\n",
    "                label=\"Mean\" if i == 0 else \"\",\n",
    "            )\n",
    "\n",
    "        ax2.legend()\n",
    "    else:\n",
    "        ax2 = axes[1]\n",
    "        ax2.text(\n",
    "            0.5,\n",
    "            0.5,\n",
    "            \"No LOOCV results available\",\n",
    "            ha=\"center\",\n",
    "            va=\"center\",\n",
    "            transform=ax2.transAxes,\n",
    "            fontsize=14,\n",
    "        )\n",
    "        ax2.set_title(\"LOOCV Results - Not Available\", fontsize=14)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No successful hierarchical models to plot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e047e1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed comparison: Hierarchical vs Individual Basin Models\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"HIERARCHICAL VS INDIVIDUAL BASIN MODEL COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Compare with individual basin models (from previous analysis)\n",
    "if \"loocv_df\" in locals():\n",
    "    # Calculate statistics for individual basin models\n",
    "    individual_stats = {\n",
    "        \"SWE Individual\": {\n",
    "            \"mean_r2\": loocv_df[\"r2_swe_clipped\"].mean(),\n",
    "            \"median_r2\": loocv_df[\"r2_swe_clipped\"].median(),\n",
    "            \"std_r2\": loocv_df[\"r2_swe_clipped\"].std(),\n",
    "            \"n_basins\": len(loocv_df),\n",
    "        }\n",
    "    }\n",
    "\n",
    "    print(\"\\nIndividual Basin Models (LOOCV):\")\n",
    "    for model, stats in individual_stats.items():\n",
    "        print(f\"  {model}:\")\n",
    "        print(f\"    Mean R² = {stats['mean_r2']:.4f} ± {stats['std_r2']:.4f}\")\n",
    "        print(f\"    Median R² = {stats['median_r2']:.4f}\")\n",
    "        print(f\"    N basins = {stats['n_basins']}\")\n",
    "\n",
    "# Hierarchical model statistics\n",
    "if len(loocv_hierarchical_results) > 0:\n",
    "    print(\"\\nHierarchical Models (LOOCV):\")\n",
    "    for result in loocv_hierarchical_results:\n",
    "        print(f\"  {result['model_name']}:\")\n",
    "        print(\n",
    "            f\"    Mean R² = {result['loocv_r2_mean']:.4f} ± {result['loocv_r2_std']:.4f}\"\n",
    "        )\n",
    "        print(f\"    Median R² = {result['loocv_r2_median']:.4f}\")\n",
    "        print(f\"    N folds = {result['n_folds']}\")\n",
    "\n",
    "# Model interpretation: Display fixed and random effects for best model\n",
    "if len(successful_hier_results) > 0:\n",
    "    # Find best model based on full data R²\n",
    "    best_model = max(successful_hier_results, key=lambda x: x[\"r2\"])\n",
    "\n",
    "    print(f\"\\n\" + \"=\" * 60)\n",
    "    print(f\"BEST HIERARCHICAL MODEL: {best_model['model_name']}\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"R² = {best_model['r2']:.4f}\")\n",
    "    print(f\"AIC = {best_model['aic']:.2f}\")\n",
    "    print(f\"BIC = {best_model['bic']:.2f}\")\n",
    "\n",
    "    if best_model[\"model\"] is not None:\n",
    "        print(f\"\\nModel Summary:\")\n",
    "        print(best_model[\"model\"].summary())\n",
    "\n",
    "# Save hierarchical results\n",
    "if len(hierarchical_results) > 0:\n",
    "    # Prepare results for saving\n",
    "    hier_results_df = pd.DataFrame(\n",
    "        [\n",
    "            {\n",
    "                \"model_name\": r[\"model_name\"],\n",
    "                \"fixed_formula\": r[\"fixed_formula\"],\n",
    "                \"random_formula\": r[\"random_formula\"],\n",
    "                \"full_data_r2\": r[\"r2\"],\n",
    "                \"aic\": r[\"aic\"],\n",
    "                \"bic\": r[\"bic\"],\n",
    "                \"success\": r[\"success\"],\n",
    "            }\n",
    "            for r in hierarchical_results\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Add LOOCV results if available\n",
    "    if len(loocv_hierarchical_results) > 0:\n",
    "        loocv_dict = {r[\"model_name\"]: r for r in loocv_hierarchical_results}\n",
    "        hier_results_df[\"loocv_mean_r2\"] = hier_results_df[\"model_name\"].map(\n",
    "            lambda x: loocv_dict.get(x, {}).get(\"loocv_r2_mean\", None)\n",
    "        )\n",
    "        hier_results_df[\"loocv_std_r2\"] = hier_results_df[\"model_name\"].map(\n",
    "            lambda x: loocv_dict.get(x, {}).get(\"loocv_r2_std\", None)\n",
    "        )\n",
    "        hier_results_df[\"loocv_n_folds\"] = hier_results_df[\"model_name\"].map(\n",
    "            lambda x: loocv_dict.get(x, {}).get(\"n_folds\", None)\n",
    "        )\n",
    "\n",
    "    # Save to CSV\n",
    "    hier_results_df.to_csv(\"hierarchical_regression_results.csv\", index=False)\n",
    "    print(\n",
    "        f\"\\nHierarchical model results saved to 'hierarchical_regression_results.csv'\"\n",
    "    )\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(\"ANALYSIS COMPLETE\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f0d42a",
   "metadata": {},
   "source": [
    "## Per-Basin R² Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44792a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate per-basin R² values for hierarchical models\n",
    "def calculate_basin_r2_hierarchical(data, model_result, model_name):\n",
    "    \"\"\"\n",
    "    Calculate R² for each basin separately using hierarchical model predictions.\n",
    "\n",
    "    Args:\n",
    "        data: DataFrame with features, target, and basin codes\n",
    "        model_result: Fitted hierarchical model result\n",
    "        model_name: Name of the model\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with R² for each basin\n",
    "    \"\"\"\n",
    "    basin_r2_results = []\n",
    "    unique_codes = data[\"code\"].unique()\n",
    "\n",
    "    # Get model predictions\n",
    "    predictions = model_result.fittedvalues\n",
    "\n",
    "    for code in unique_codes:\n",
    "        basin_mask = data[\"code\"] == code\n",
    "        basin_data = data[basin_mask]\n",
    "\n",
    "        if len(basin_data) < 3:  # Skip basins with too few data points\n",
    "            continue\n",
    "\n",
    "        # Get predictions and actual values for this basin\n",
    "        y_true = basin_data[\"target\"].values\n",
    "        y_pred = predictions[basin_mask].values\n",
    "\n",
    "        # Calculate R² for this basin\n",
    "        r2_basin = r2_score(y_true, y_pred)\n",
    "\n",
    "        basin_r2_results.append(\n",
    "            {\n",
    "                \"code\": code,\n",
    "                \"model_name\": model_name,\n",
    "                \"r2\": r2_basin,\n",
    "                \"r2_clipped\": np.clip(r2_basin, -1, 1),\n",
    "                \"n_samples\": len(y_true),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return pd.DataFrame(basin_r2_results)\n",
    "\n",
    "\n",
    "# Calculate per-basin R² for all successful hierarchical models\n",
    "all_basin_r2_results = []\n",
    "\n",
    "for result in hierarchical_results:\n",
    "    if result[\"success\"] and result[\"model\"] is not None:\n",
    "        model_name = result[\"model_name\"]\n",
    "        model_result = result[\"model\"]\n",
    "\n",
    "        print(f\"Calculating per-basin R² for {model_name}...\")\n",
    "\n",
    "        basin_r2_df = calculate_basin_r2_hierarchical(\n",
    "            hier_data_processed, model_result, model_name\n",
    "        )\n",
    "\n",
    "        all_basin_r2_results.append(basin_r2_df)\n",
    "\n",
    "        print(f\"  Calculated R² for {len(basin_r2_df)} basins\")\n",
    "        print(f\"  Mean basin R² = {basin_r2_df['r2_clipped'].mean():.4f}\")\n",
    "        print(f\"  Std basin R² = {basin_r2_df['r2_clipped'].std():.4f}\")\n",
    "\n",
    "# Combine all basin R² results\n",
    "if len(all_basin_r2_results) > 0:\n",
    "    basin_r2_combined = pd.concat(all_basin_r2_results, ignore_index=True)\n",
    "    print(f\"\\nTotal basin-model combinations: {len(basin_r2_combined)}\")\n",
    "    print(f\"Unique basins: {basin_r2_combined['code'].nunique()}\")\n",
    "    print(f\"Unique models: {basin_r2_combined['model_name'].nunique()}\")\n",
    "else:\n",
    "    print(\"No successful hierarchical models found for basin-level analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b8f6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize R² distributions for each basin across different hierarchical models\n",
    "if len(all_basin_r2_results) > 0:\n",
    "    # Create histogram plots for each model showing basin R² distribution\n",
    "    n_models = len(all_basin_r2_results)\n",
    "\n",
    "    if n_models > 0:\n",
    "        fig, axes = plt.subplots(n_models, 1, figsize=(12, 4 * n_models))\n",
    "\n",
    "        # Ensure axes is always iterable\n",
    "        if n_models == 1:\n",
    "            axes = [axes]\n",
    "\n",
    "        colors = [\"steelblue\", \"forestgreen\", \"coral\", \"purple\", \"orange\"]\n",
    "\n",
    "        for i, basin_df in enumerate(all_basin_r2_results):\n",
    "            ax = axes[i]\n",
    "            model_name = basin_df[\"model_name\"].iloc[0]\n",
    "\n",
    "            # Plot histogram of basin R² values\n",
    "            r2_values = basin_df[\"r2_clipped\"]\n",
    "\n",
    "            ax.hist(\n",
    "                r2_values,\n",
    "                bins=20,\n",
    "                alpha=0.7,\n",
    "                color=colors[i % len(colors)],\n",
    "                edgecolor=\"black\",\n",
    "                density=False,\n",
    "            )\n",
    "\n",
    "            # Add statistics\n",
    "            mean_r2 = r2_values.mean()\n",
    "            median_r2 = r2_values.median()\n",
    "            std_r2 = r2_values.std()\n",
    "\n",
    "            ax.axvline(\n",
    "                mean_r2,\n",
    "                color=\"red\",\n",
    "                linestyle=\"--\",\n",
    "                linewidth=2,\n",
    "                label=f\"Mean: {mean_r2:.3f}\",\n",
    "            )\n",
    "            ax.axvline(\n",
    "                median_r2,\n",
    "                color=\"orange\",\n",
    "                linestyle=\"--\",\n",
    "                linewidth=2,\n",
    "                label=f\"Median: {median_r2:.3f}\",\n",
    "            )\n",
    "\n",
    "            # Formatting\n",
    "            ax.set_title(\n",
    "                f\"{model_name} - Basin R² Distribution (n={len(r2_values)} basins)\",\n",
    "                fontsize=12,\n",
    "                fontweight=\"bold\",\n",
    "            )\n",
    "            ax.set_xlabel(\"R² Score per Basin (clipped to [-1, 1])\", fontsize=10)\n",
    "            ax.set_ylabel(\"Number of Basins\", fontsize=10)\n",
    "            ax.set_xlim(-1, 1)\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            ax.legend(fontsize=10)\n",
    "\n",
    "            # Add statistics text box\n",
    "            stats_text = (\n",
    "                f\"Mean: {mean_r2:.3f}\\n\"\n",
    "                f\"Median: {median_r2:.3f}\\n\"\n",
    "                f\"Std: {std_r2:.3f}\\n\"\n",
    "                f\"Min: {r2_values.min():.3f}\\n\"\n",
    "                f\"Max: {r2_values.max():.3f}\"\n",
    "            )\n",
    "\n",
    "            ax.text(\n",
    "                0.02,\n",
    "                0.98,\n",
    "                stats_text,\n",
    "                transform=ax.transAxes,\n",
    "                fontsize=9,\n",
    "                verticalalignment=\"top\",\n",
    "                bbox=dict(boxstyle=\"round\", facecolor=\"lightblue\", alpha=0.8),\n",
    "            )\n",
    "\n",
    "        plt.suptitle(\n",
    "            \"Hierarchical Models: R² Distribution Across Basins\",\n",
    "            fontsize=16,\n",
    "            fontweight=\"bold\",\n",
    "        )\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"No basin R² data available for plotting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358dd904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare hierarchical model performance vs individual basin models\n",
    "if len(all_basin_r2_results) > 0:\n",
    "    # Create comparison plot: Hierarchical vs Individual Basin Models\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "    # Plot 1: Individual basin model R² distribution (from previous LOOCV analysis)\n",
    "    if \"loocv_df\" in locals():\n",
    "        ax1 = axes[0]\n",
    "\n",
    "        # Plot SWE individual model results\n",
    "        swe_r2 = loocv_df[\"r2_swe_clipped\"]\n",
    "\n",
    "        ax1.hist(\n",
    "            swe_r2,\n",
    "            bins=20,\n",
    "            alpha=0.7,\n",
    "            color=\"steelblue\",\n",
    "            edgecolor=\"black\",\n",
    "            label=f\"Individual SWE Models (n={len(swe_r2)})\",\n",
    "        )\n",
    "\n",
    "        ax1.axvline(\n",
    "            swe_r2.mean(),\n",
    "            color=\"red\",\n",
    "            linestyle=\"--\",\n",
    "            linewidth=2,\n",
    "            label=f\"Mean: {swe_r2.mean():.3f}\",\n",
    "        )\n",
    "        ax1.axvline(\n",
    "            swe_r2.median(),\n",
    "            color=\"orange\",\n",
    "            linestyle=\"--\",\n",
    "            linewidth=2,\n",
    "            label=f\"Median: {swe_r2.median():.3f}\",\n",
    "        )\n",
    "\n",
    "        ax1.set_title(\n",
    "            \"Individual Basin Models\\n(LOOCV R² Distribution)\",\n",
    "            fontsize=12,\n",
    "            fontweight=\"bold\",\n",
    "        )\n",
    "        ax1.set_xlabel(\"R² Score (clipped to [-1, 1])\", fontsize=10)\n",
    "        ax1.set_ylabel(\"Number of Basins\", fontsize=10)\n",
    "        ax1.set_xlim(-1, 1)\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        ax1.legend(fontsize=9)\n",
    "\n",
    "    # Plot 2: Best hierarchical model R² distribution\n",
    "    ax2 = axes[1]\n",
    "\n",
    "    if len(all_basin_r2_results) > 0:\n",
    "        # Find the model with highest mean basin R²\n",
    "        model_means = []\n",
    "        for basin_df in all_basin_r2_results:\n",
    "            model_name = basin_df[\"model_name\"].iloc[0]\n",
    "            mean_r2 = basin_df[\"r2_clipped\"].mean()\n",
    "            model_means.append((model_name, mean_r2, basin_df))\n",
    "\n",
    "        best_model_name, best_mean_r2, best_basin_df = max(\n",
    "            model_means, key=lambda x: x[1]\n",
    "        )\n",
    "\n",
    "        hier_r2 = best_basin_df[\"r2_clipped\"]\n",
    "\n",
    "        ax2.hist(\n",
    "            hier_r2,\n",
    "            bins=20,\n",
    "            alpha=0.7,\n",
    "            color=\"forestgreen\",\n",
    "            edgecolor=\"black\",\n",
    "            label=f\"Hierarchical Model (n={len(hier_r2)})\",\n",
    "        )\n",
    "\n",
    "        ax2.axvline(\n",
    "            hier_r2.mean(),\n",
    "            color=\"red\",\n",
    "            linestyle=\"--\",\n",
    "            linewidth=2,\n",
    "            label=f\"Mean: {hier_r2.mean():.3f}\",\n",
    "        )\n",
    "        ax2.axvline(\n",
    "            hier_r2.median(),\n",
    "            color=\"orange\",\n",
    "            linestyle=\"--\",\n",
    "            linewidth=2,\n",
    "            label=f\"Median: {hier_r2.median():.3f}\",\n",
    "        )\n",
    "\n",
    "        ax2.set_title(\n",
    "            f\"Best Hierarchical Model: {best_model_name}\\n(Basin R² Distribution)\",\n",
    "            fontsize=12,\n",
    "            fontweight=\"bold\",\n",
    "        )\n",
    "        ax2.set_xlabel(\"R² Score (clipped to [-1, 1])\", fontsize=10)\n",
    "        ax2.set_ylabel(\"Number of Basins\", fontsize=10)\n",
    "        ax2.set_xlim(-1, 1)\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        ax2.legend(fontsize=9)\n",
    "\n",
    "    plt.suptitle(\n",
    "        \"Model Performance Comparison: Individual vs Hierarchical\",\n",
    "        fontsize=16,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Print detailed comparison statistics\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"DETAILED BASIN-LEVEL PERFORMANCE COMPARISON\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    if \"loocv_df\" in locals():\n",
    "        individual_r2 = loocv_df[\"r2_swe_clipped\"]\n",
    "        print(f\"\\nIndividual Basin Models (SWE LOOCV):\")\n",
    "        print(f\"  Mean R² = {individual_r2.mean():.4f} ± {individual_r2.std():.4f}\")\n",
    "        print(f\"  Median R² = {individual_r2.median():.4f}\")\n",
    "        print(f\"  Range: [{individual_r2.min():.4f}, {individual_r2.max():.4f}]\")\n",
    "        print(f\"  N basins = {len(individual_r2)}\")\n",
    "\n",
    "    print(f\"\\nHierarchical Models (Basin-level R²):\")\n",
    "    for basin_df in all_basin_r2_results:\n",
    "        model_name = basin_df[\"model_name\"].iloc[0]\n",
    "        r2_values = basin_df[\"r2_clipped\"]\n",
    "\n",
    "        print(f\"  {model_name}:\")\n",
    "        print(f\"    Mean R² = {r2_values.mean():.4f} ± {r2_values.std():.4f}\")\n",
    "        print(f\"    Median R² = {r2_values.median():.4f}\")\n",
    "        print(f\"    Range: [{r2_values.min():.4f}, {r2_values.max():.4f}]\")\n",
    "        print(f\"    N basins = {len(r2_values)}\")\n",
    "\n",
    "        # Performance improvement calculation\n",
    "        if \"loocv_df\" in locals():\n",
    "            improvement = r2_values.mean() - individual_r2.mean()\n",
    "            print(f\"    Improvement over individual: {improvement:+.4f}\")\n",
    "else:\n",
    "    print(\"No hierarchical basin R² results available for comparison\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3e7b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save detailed basin-level results\n",
    "if len(all_basin_r2_results) > 0:\n",
    "    # Save basin R² results to CSV\n",
    "    basin_r2_combined.to_csv(\"hierarchical_basin_r2_results.csv\", index=False)\n",
    "    print(f\"\\nBasin-level R² results saved to 'hierarchical_basin_r2_results.csv'\")\n",
    "\n",
    "    # Create summary statistics by model\n",
    "    basin_summary_stats = (\n",
    "        basin_r2_combined.groupby(\"model_name\")[\"r2_clipped\"]\n",
    "        .agg([\"count\", \"mean\", \"median\", \"std\", \"min\", \"max\"])\n",
    "        .round(4)\n",
    "    )\n",
    "\n",
    "    print(f\"\\nBasin R² Summary Statistics by Model:\")\n",
    "    print(basin_summary_stats)\n",
    "\n",
    "    # Identify best and worst performing basins for each model\n",
    "    print(f\"\\n\" + \"=\" * 60)\n",
    "    print(\"BEST AND WORST PERFORMING BASINS\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    for model_name in basin_r2_combined[\"model_name\"].unique():\n",
    "        model_data = basin_r2_combined[basin_r2_combined[\"model_name\"] == model_name]\n",
    "\n",
    "        # Best performing basins\n",
    "        best_basins = model_data.nlargest(3, \"r2_clipped\")\n",
    "        worst_basins = model_data.nsmallest(3, \"r2_clipped\")\n",
    "\n",
    "        print(f\"\\n{model_name}:\")\n",
    "        print(f\"  Top 3 basins:\")\n",
    "        for _, row in best_basins.iterrows():\n",
    "            print(f\"    Basin {row['code']}: R² = {row['r2_clipped']:.4f}\")\n",
    "\n",
    "        print(f\"  Bottom 3 basins:\")\n",
    "        for _, row in worst_basins.iterrows():\n",
    "            print(f\"    Basin {row['code']}: R² = {row['r2_clipped']:.4f}\")\n",
    "\n",
    "# Create basin performance heatmap across models\n",
    "if len(all_basin_r2_results) > 1:\n",
    "    # Pivot table for heatmap\n",
    "    pivot_data = basin_r2_combined.pivot(\n",
    "        index=\"code\", columns=\"model_name\", values=\"r2_clipped\"\n",
    "    )\n",
    "\n",
    "    # Only show basins that have data for all models\n",
    "    complete_basins = pivot_data.dropna()\n",
    "\n",
    "    if len(complete_basins) > 0:\n",
    "        plt.figure(figsize=(12, max(6, len(complete_basins) * 0.3)))\n",
    "\n",
    "        # Create heatmap\n",
    "        sns.heatmap(\n",
    "            complete_basins,\n",
    "            annot=True,\n",
    "            cmap=\"RdYlBu_r\",\n",
    "            center=0.5,\n",
    "            fmt=\".3f\",\n",
    "            cbar_kws={\"label\": \"R² Score\"},\n",
    "            vmin=-1,\n",
    "            vmax=1,\n",
    "        )\n",
    "\n",
    "        plt.title(\n",
    "            \"Basin Performance Across Hierarchical Models\\n(R² Heatmap)\",\n",
    "            fontsize=14,\n",
    "            fontweight=\"bold\",\n",
    "        )\n",
    "        plt.xlabel(\"Model Type\", fontsize=12)\n",
    "        plt.ylabel(\"Basin Code\", fontsize=12)\n",
    "        plt.xticks(rotation=45, ha=\"right\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        print(\n",
    "            f\"\\nHeatmap shows {len(complete_basins)} basins with complete data across all models\"\n",
    "        )\n",
    "    else:\n",
    "        print(\n",
    "            \"No basins have complete data across all models for heatmap visualization\"\n",
    "        )\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(\"BASIN-LEVEL ANALYSIS COMPLETE\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5597476a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "08059f1b",
   "metadata": {},
   "source": [
    "## Spatial Correlation Analysis Between Basins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad773cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create spatial correlation matrix between basin discharge targets\n",
    "# This analysis will show if there are years where all basins tend to have similarly high or low discharge\n",
    "\n",
    "# Prepare data: Create a matrix with basins as columns and years as rows\n",
    "print(\"Creating spatial correlation matrix between basin discharge targets...\")\n",
    "print(f\"Available data shape: {data_march.shape}\")\n",
    "print(f\"Available basins: {data_march['code'].nunique()}\")\n",
    "print(f\"Date range: {data_march['date'].min()} to {data_march['date'].max()}\")\n",
    "\n",
    "# Create a pivot table: years as rows, basins as columns, discharge as values\n",
    "discharge_pivot = data_march.pivot_table(\n",
    "    index=data_march[\"date\"].dt.year,\n",
    "    columns=\"code\",\n",
    "    values=\"target\",\n",
    "    aggfunc=\"mean\",  # Average if multiple values per year per basin\n",
    ")\n",
    "\n",
    "print(f\"\\nPivot table shape: {discharge_pivot.shape}\")\n",
    "print(f\"Years: {discharge_pivot.index.min()} to {discharge_pivot.index.max()}\")\n",
    "print(f\"Number of basins with data: {discharge_pivot.columns.nunique()}\")\n",
    "\n",
    "# Remove basins with too much missing data (less than 50% coverage)\n",
    "min_years_required = len(discharge_pivot) * 0.5\n",
    "basins_with_sufficient_data = discharge_pivot.columns[\n",
    "    discharge_pivot.count() >= min_years_required\n",
    "]\n",
    "\n",
    "print(\n",
    "    f\"\\nBasins with sufficient data (>50% coverage): {len(basins_with_sufficient_data)}\"\n",
    ")\n",
    "\n",
    "# Filter to basins with sufficient data\n",
    "discharge_matrix = discharge_pivot[basins_with_sufficient_data].copy()\n",
    "\n",
    "print(f\"Final data matrix shape: {discharge_matrix.shape}\")\n",
    "print(\n",
    "    f\"Data completeness: {(1 - discharge_matrix.isnull().sum().sum() / discharge_matrix.size) * 100:.1f}%\"\n",
    ")\n",
    "\n",
    "# Display first few rows and columns\n",
    "print(f\"\\nFirst few rows and columns of discharge matrix:\")\n",
    "print(discharge_matrix.iloc[:5, :5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110df0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate pairwise correlation matrix between basins\n",
    "print(\"Calculating pairwise correlations between basins...\")\n",
    "\n",
    "# Calculate correlation matrix (Pearson correlation)\n",
    "basin_correlation_matrix = discharge_matrix.corr(method=\"pearson\")\n",
    "\n",
    "# Also calculate Spearman correlation for comparison (rank-based)\n",
    "basin_correlation_spearman = discharge_matrix.corr(method=\"spearman\")\n",
    "\n",
    "print(f\"Correlation matrix shape: {basin_correlation_matrix.shape}\")\n",
    "print(\n",
    "    f\"Number of basin pairs: {(len(basin_correlation_matrix) * (len(basin_correlation_matrix) - 1)) // 2}\"\n",
    ")\n",
    "\n",
    "# Summary statistics of correlations\n",
    "correlation_values = basin_correlation_matrix.values\n",
    "# Remove diagonal (self-correlations of 1.0)\n",
    "off_diagonal_correlations = correlation_values[\n",
    "    np.triu_indices_from(correlation_values, k=1)\n",
    "]\n",
    "\n",
    "print(f\"\\nPearson Correlation Statistics:\")\n",
    "print(f\"  Mean correlation: {np.nanmean(off_diagonal_correlations):.3f}\")\n",
    "print(f\"  Median correlation: {np.nanmedian(off_diagonal_correlations):.3f}\")\n",
    "print(f\"  Std deviation: {np.nanstd(off_diagonal_correlations):.3f}\")\n",
    "print(f\"  Min correlation: {np.nanmin(off_diagonal_correlations):.3f}\")\n",
    "print(f\"  Max correlation: {np.nanmax(off_diagonal_correlations):.3f}\")\n",
    "\n",
    "# Count strong correlations\n",
    "strong_positive = np.sum(off_diagonal_correlations > 0.7)\n",
    "moderate_positive = np.sum(\n",
    "    (off_diagonal_correlations > 0.3) & (off_diagonal_correlations <= 0.7)\n",
    ")\n",
    "weak_correlations = np.sum(\n",
    "    (off_diagonal_correlations >= -0.3) & (off_diagonal_correlations <= 0.3)\n",
    ")\n",
    "negative_correlations = np.sum(off_diagonal_correlations < -0.3)\n",
    "\n",
    "total_pairs = len(off_diagonal_correlations)\n",
    "print(f\"\\nCorrelation Distribution:\")\n",
    "print(\n",
    "    f\"  Strong positive (>0.7): {strong_positive} ({strong_positive / total_pairs * 100:.1f}%)\"\n",
    ")\n",
    "print(\n",
    "    f\"  Moderate positive (0.3-0.7): {moderate_positive} ({moderate_positive / total_pairs * 100:.1f}%)\"\n",
    ")\n",
    "print(\n",
    "    f\"  Weak (-0.3 to 0.3): {weak_correlations} ({weak_correlations / total_pairs * 100:.1f}%)\"\n",
    ")\n",
    "print(\n",
    "    f\"  Negative (<-0.3): {negative_correlations} ({negative_correlations / total_pairs * 100:.1f}%)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0759facc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the spatial correlation matrix with heatmap\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 16))\n",
    "\n",
    "# Plot 1: Full correlation matrix heatmap (Pearson)\n",
    "ax1 = axes[0, 0]\n",
    "mask = np.triu(\n",
    "    np.ones_like(basin_correlation_matrix, dtype=bool), k=1\n",
    ")  # Mask upper triangle\n",
    "sns.heatmap(\n",
    "    basin_correlation_matrix,\n",
    "    mask=mask,\n",
    "    annot=False,\n",
    "    cmap=\"RdBu_r\",\n",
    "    center=0,\n",
    "    square=True,\n",
    "    ax=ax1,\n",
    "    cbar_kws={\"label\": \"Pearson Correlation\"},\n",
    "    vmin=-1,\n",
    "    vmax=1,\n",
    ")\n",
    "ax1.set_title(\n",
    "    \"Basin Discharge Spatial Correlations\\n(Pearson - Lower Triangle)\",\n",
    "    fontsize=12,\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "ax1.set_xlabel(\"Basin Code\", fontsize=10)\n",
    "ax1.set_ylabel(\"Basin Code\", fontsize=10)\n",
    "\n",
    "# Plot 2: Correlation distribution histogram\n",
    "ax2 = axes[0, 1]\n",
    "ax2.hist(\n",
    "    off_diagonal_correlations, bins=30, alpha=0.7, color=\"steelblue\", edgecolor=\"black\"\n",
    ")\n",
    "ax2.axvline(\n",
    "    np.nanmean(off_diagonal_correlations),\n",
    "    color=\"red\",\n",
    "    linestyle=\"--\",\n",
    "    linewidth=2,\n",
    "    label=f\"Mean: {np.nanmean(off_diagonal_correlations):.3f}\",\n",
    ")\n",
    "ax2.axvline(\n",
    "    np.nanmedian(off_diagonal_correlations),\n",
    "    color=\"orange\",\n",
    "    linestyle=\"--\",\n",
    "    linewidth=2,\n",
    "    label=f\"Median: {np.nanmedian(off_diagonal_correlations):.3f}\",\n",
    ")\n",
    "ax2.set_title(\n",
    "    \"Distribution of Pairwise Correlations\\nBetween Basin Discharge\",\n",
    "    fontsize=12,\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "ax2.set_xlabel(\"Correlation Coefficient\", fontsize=10)\n",
    "ax2.set_ylabel(\"Frequency\", fontsize=10)\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Spearman correlation matrix for comparison\n",
    "ax3 = axes[1, 0]\n",
    "mask_spearman = np.triu(np.ones_like(basin_correlation_spearman, dtype=bool), k=1)\n",
    "sns.heatmap(\n",
    "    basin_correlation_spearman,\n",
    "    mask=mask_spearman,\n",
    "    annot=False,\n",
    "    cmap=\"RdBu_r\",\n",
    "    center=0,\n",
    "    square=True,\n",
    "    ax=ax3,\n",
    "    cbar_kws={\"label\": \"Spearman Correlation\"},\n",
    "    vmin=-1,\n",
    "    vmax=1,\n",
    ")\n",
    "ax3.set_title(\n",
    "    \"Basin Discharge Spatial Correlations\\n(Spearman - Lower Triangle)\",\n",
    "    fontsize=12,\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "ax3.set_xlabel(\"Basin Code\", fontsize=10)\n",
    "ax3.set_ylabel(\"Basin Code\", fontsize=10)\n",
    "\n",
    "# Plot 4: Correlation vs Geographic Distance (if we had coordinates)\n",
    "ax4 = axes[1, 1]\n",
    "# For now, we'll show a scatter plot of correlation strength vs basin ID difference\n",
    "# This gives a rough proxy for potential geographic effects\n",
    "basin_ids = basin_correlation_matrix.columns.values\n",
    "correlation_pairs = []\n",
    "id_differences = []\n",
    "\n",
    "for i in range(len(basin_ids)):\n",
    "    for j in range(i + 1, len(basin_ids)):\n",
    "        basin_i, basin_j = basin_ids[i], basin_ids[j]\n",
    "        correlation = basin_correlation_matrix.loc[basin_i, basin_j]\n",
    "        if not np.isnan(correlation):\n",
    "            correlation_pairs.append(abs(correlation))  # Use absolute correlation\n",
    "            id_differences.append(abs(basin_i - basin_j))\n",
    "\n",
    "ax4.scatter(id_differences, correlation_pairs, alpha=0.6, s=20)\n",
    "ax4.set_xlabel(\"Basin ID Difference (Proxy for Distance)\", fontsize=10)\n",
    "ax4.set_ylabel(\"Absolute Correlation\", fontsize=10)\n",
    "ax4.set_title(\n",
    "    \"Correlation Strength vs Basin ID Difference\\n(Proxy for Spatial Distance)\",\n",
    "    fontsize=12,\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "# Add trend line\n",
    "if len(correlation_pairs) > 0:\n",
    "    z = np.polyfit(id_differences, correlation_pairs, 1)\n",
    "    p = np.poly1d(z)\n",
    "    ax4.plot(\n",
    "        id_differences,\n",
    "        p(id_differences),\n",
    "        \"r--\",\n",
    "        alpha=0.8,\n",
    "        label=f\"Trend: slope={z[0]:.4f}\",\n",
    "    )\n",
    "    ax4.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63feb377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify clusters of highly correlated basins\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "from scipy.spatial.distance import squareform\n",
    "\n",
    "print(\"Performing hierarchical clustering of basins based on discharge correlations...\")\n",
    "\n",
    "# Convert correlation matrix to distance matrix (1 - correlation)\n",
    "distance_matrix = 1 - basin_correlation_matrix.abs()\n",
    "\n",
    "# Handle NaN values by setting them to maximum distance\n",
    "distance_matrix = distance_matrix.fillna(1.0)\n",
    "\n",
    "# Perform hierarchical clustering\n",
    "linkage_matrix = linkage(squareform(distance_matrix), method=\"ward\")\n",
    "\n",
    "# Create dendrogram plot\n",
    "fig, axes = plt.subplots(2, 1, figsize=(15, 12))\n",
    "\n",
    "# Plot dendrogram\n",
    "ax1 = axes[0]\n",
    "dendrogram(\n",
    "    linkage_matrix,\n",
    "    labels=basin_correlation_matrix.columns.astype(str),\n",
    "    ax=ax1,\n",
    "    leaf_rotation=90,\n",
    "    leaf_font_size=8,\n",
    ")\n",
    "ax1.set_title(\n",
    "    \"Hierarchical Clustering of Basins\\n(Based on Discharge Correlations)\",\n",
    "    fontsize=14,\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "ax1.set_xlabel(\"Basin Code\", fontsize=12)\n",
    "ax1.set_ylabel(\"Distance (1 - |Correlation|)\", fontsize=12)\n",
    "\n",
    "# Get cluster assignments (try different numbers of clusters)\n",
    "n_clusters = 4\n",
    "cluster_labels = fcluster(linkage_matrix, n_clusters, criterion=\"maxclust\")\n",
    "\n",
    "# Create a mapping of basin to cluster\n",
    "basin_to_cluster = dict(zip(basin_correlation_matrix.columns, cluster_labels))\n",
    "\n",
    "print(f\"\\nCluster assignments (k={n_clusters}):\")\n",
    "for cluster_id in range(1, n_clusters + 1):\n",
    "    basins_in_cluster = [\n",
    "        basin for basin, cluster in basin_to_cluster.items() if cluster == cluster_id\n",
    "    ]\n",
    "    print(f\"  Cluster {cluster_id}: {basins_in_cluster} (n={len(basins_in_cluster)})\")\n",
    "\n",
    "# Plot clustered correlation matrix\n",
    "ax2 = axes[1]\n",
    "# Reorder the correlation matrix by cluster\n",
    "cluster_order = []\n",
    "for cluster_id in range(1, n_clusters + 1):\n",
    "    basins_in_cluster = [\n",
    "        basin for basin, cluster in basin_to_cluster.items() if cluster == cluster_id\n",
    "    ]\n",
    "    cluster_order.extend(sorted(basins_in_cluster))\n",
    "\n",
    "clustered_corr_matrix = basin_correlation_matrix.loc[cluster_order, cluster_order]\n",
    "\n",
    "# Create heatmap with cluster boundaries\n",
    "im = ax2.imshow(\n",
    "    clustered_corr_matrix.values, cmap=\"RdBu_r\", vmin=-1, vmax=1, aspect=\"auto\"\n",
    ")\n",
    "\n",
    "# Add cluster boundaries\n",
    "cluster_boundaries = []\n",
    "current_pos = 0\n",
    "for cluster_id in range(1, n_clusters + 1):\n",
    "    basins_in_cluster = [\n",
    "        basin for basin, cluster in basin_to_cluster.items() if cluster == cluster_id\n",
    "    ]\n",
    "    current_pos += len(basins_in_cluster)\n",
    "    if cluster_id < n_clusters:  # Don't add line after last cluster\n",
    "        ax2.axhline(current_pos - 0.5, color=\"black\", linewidth=2)\n",
    "        ax2.axvline(current_pos - 0.5, color=\"black\", linewidth=2)\n",
    "\n",
    "ax2.set_title(\n",
    "    f\"Clustered Correlation Matrix\\n(k={n_clusters} clusters)\",\n",
    "    fontsize=14,\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "ax2.set_xlabel(\"Basin Index (Clustered)\", fontsize=12)\n",
    "ax2.set_ylabel(\"Basin Index (Clustered)\", fontsize=12)\n",
    "\n",
    "# Add colorbar\n",
    "cbar = plt.colorbar(im, ax=ax2, shrink=0.8)\n",
    "cbar.set_label(\"Pearson Correlation\", fontsize=12)\n",
    "\n",
    "# Set tick labels\n",
    "ax2.set_xticks(range(len(cluster_order)))\n",
    "ax2.set_yticks(range(len(cluster_order)))\n",
    "ax2.set_xticklabels(cluster_order, rotation=90, fontsize=8)\n",
    "ax2.set_yticklabels(cluster_order, fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6deeb14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze temporal patterns: Identify years with synchronized high/low discharge\n",
    "print(\"Analyzing temporal synchronization patterns...\")\n",
    "\n",
    "# Calculate annual average discharge across all basins for each year\n",
    "annual_avg_discharge = discharge_matrix.mean(axis=1, skipna=True)\n",
    "annual_std_discharge = discharge_matrix.std(axis=1, skipna=True)\n",
    "\n",
    "# Identify years with particularly high or low average discharge\n",
    "discharge_threshold_high = annual_avg_discharge.quantile(0.8)  # Top 20%\n",
    "discharge_threshold_low = annual_avg_discharge.quantile(0.2)  # Bottom 20%\n",
    "\n",
    "high_discharge_years = annual_avg_discharge[\n",
    "    annual_avg_discharge >= discharge_threshold_high\n",
    "].index\n",
    "low_discharge_years = annual_avg_discharge[\n",
    "    annual_avg_discharge <= discharge_threshold_low\n",
    "].index\n",
    "\n",
    "print(f\"\\nTemporal Synchronization Analysis:\")\n",
    "print(f\"  Years with high discharge (>80th percentile): {list(high_discharge_years)}\")\n",
    "print(f\"  Years with low discharge (<20th percentile): {list(low_discharge_years)}\")\n",
    "\n",
    "# Calculate coefficient of variation across basins for each year (measure of synchronization)\n",
    "# Lower CV = more synchronized (all basins similar), Higher CV = less synchronized\n",
    "annual_cv = annual_std_discharge / annual_avg_discharge\n",
    "synchronized_years = annual_cv[\n",
    "    annual_cv <= annual_cv.quantile(0.3)\n",
    "].index  # Most synchronized 30%\n",
    "desynchronized_years = annual_cv[\n",
    "    annual_cv >= annual_cv.quantile(0.7)\n",
    "].index  # Least synchronized 30%\n",
    "\n",
    "print(f\"  Most synchronized years (low CV): {list(synchronized_years)}\")\n",
    "print(f\"  Least synchronized years (high CV): {list(desynchronized_years)}\")\n",
    "\n",
    "# Create temporal analysis plots\n",
    "fig, axes = plt.subplots(3, 1, figsize=(15, 15))\n",
    "\n",
    "# Plot 1: Time series of annual average discharge\n",
    "ax1 = axes[0]\n",
    "ax1.plot(\n",
    "    annual_avg_discharge.index,\n",
    "    annual_avg_discharge.values,\n",
    "    \"b-\",\n",
    "    linewidth=2,\n",
    "    marker=\"o\",\n",
    "    markersize=4,\n",
    ")\n",
    "ax1.axhline(\n",
    "    discharge_threshold_high,\n",
    "    color=\"red\",\n",
    "    linestyle=\"--\",\n",
    "    alpha=0.7,\n",
    "    label=\"High threshold (80th %ile)\",\n",
    ")\n",
    "ax1.axhline(\n",
    "    discharge_threshold_low,\n",
    "    color=\"orange\",\n",
    "    linestyle=\"--\",\n",
    "    alpha=0.7,\n",
    "    label=\"Low threshold (20th %ile)\",\n",
    ")\n",
    "\n",
    "# Highlight extreme years\n",
    "ax1.scatter(\n",
    "    high_discharge_years,\n",
    "    annual_avg_discharge[high_discharge_years],\n",
    "    color=\"red\",\n",
    "    s=60,\n",
    "    zorder=5,\n",
    "    label=\"High discharge years\",\n",
    ")\n",
    "ax1.scatter(\n",
    "    low_discharge_years,\n",
    "    annual_avg_discharge[low_discharge_years],\n",
    "    color=\"orange\",\n",
    "    s=60,\n",
    "    zorder=5,\n",
    "    label=\"Low discharge years\",\n",
    ")\n",
    "\n",
    "ax1.set_title(\n",
    "    \"Annual Average Discharge Across All Basins (March)\", fontsize=14, fontweight=\"bold\"\n",
    ")\n",
    "ax1.set_xlabel(\"Year\", fontsize=12)\n",
    "ax1.set_ylabel(\"Average Discharge\", fontsize=12)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.legend()\n",
    "\n",
    "# Plot 2: Coefficient of variation over time\n",
    "ax2 = axes[1]\n",
    "ax2.plot(annual_cv.index, annual_cv.values, \"g-\", linewidth=2, marker=\"s\", markersize=4)\n",
    "ax2.axhline(\n",
    "    annual_cv.quantile(0.3),\n",
    "    color=\"blue\",\n",
    "    linestyle=\"--\",\n",
    "    alpha=0.7,\n",
    "    label=\"High sync threshold (30th %ile)\",\n",
    ")\n",
    "ax2.axhline(\n",
    "    annual_cv.quantile(0.7),\n",
    "    color=\"purple\",\n",
    "    linestyle=\"--\",\n",
    "    alpha=0.7,\n",
    "    label=\"Low sync threshold (70th %ile)\",\n",
    ")\n",
    "\n",
    "# Highlight synchronized/desynchronized years\n",
    "ax2.scatter(\n",
    "    synchronized_years,\n",
    "    annual_cv[synchronized_years],\n",
    "    color=\"blue\",\n",
    "    s=60,\n",
    "    zorder=5,\n",
    "    label=\"Synchronized years\",\n",
    ")\n",
    "ax2.scatter(\n",
    "    desynchronized_years,\n",
    "    annual_cv[desynchronized_years],\n",
    "    color=\"purple\",\n",
    "    s=60,\n",
    "    zorder=5,\n",
    "    label=\"Desynchronized years\",\n",
    ")\n",
    "\n",
    "ax2.set_title(\n",
    "    \"Inter-Basin Synchronization Over Time\\n(Coefficient of Variation - Lower = More Synchronized)\",\n",
    "    fontsize=14,\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "ax2.set_xlabel(\"Year\", fontsize=12)\n",
    "ax2.set_ylabel(\"Coefficient of Variation\", fontsize=12)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.legend()\n",
    "\n",
    "# Plot 3: Heatmap of discharge by year and basin\n",
    "ax3 = axes[2]\n",
    "# Normalize discharge for better visualization (z-score by basin)\n",
    "discharge_normalized = discharge_matrix.sub(discharge_matrix.mean(axis=0), axis=1).div(\n",
    "    discharge_matrix.std(axis=0), axis=1\n",
    ")\n",
    "\n",
    "im = ax3.imshow(\n",
    "    discharge_normalized.T.values, cmap=\"RdBu_r\", aspect=\"auto\", vmin=-3, vmax=3\n",
    ")\n",
    "ax3.set_title(\n",
    "    \"Normalized Discharge Patterns by Year and Basin\\n(Blue = Below Average, Red = Above Average)\",\n",
    "    fontsize=14,\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "ax3.set_xlabel(\"Year Index\", fontsize=12)\n",
    "ax3.set_ylabel(\"Basin Code\", fontsize=12)\n",
    "\n",
    "# Set tick labels\n",
    "years = discharge_normalized.index.values\n",
    "ax3.set_xticks(range(0, len(years), max(1, len(years) // 10)))\n",
    "ax3.set_xticklabels(\n",
    "    [years[i] for i in range(0, len(years), max(1, len(years) // 10))], rotation=45\n",
    ")\n",
    "\n",
    "basins = discharge_normalized.columns.values\n",
    "ax3.set_yticks(range(0, len(basins), max(1, len(basins) // 10)))\n",
    "ax3.set_yticklabels(\n",
    "    [basins[i] for i in range(0, len(basins), max(1, len(basins) // 10))]\n",
    ")\n",
    "\n",
    "# Add colorbar\n",
    "cbar = plt.colorbar(im, ax=ax3, shrink=0.8)\n",
    "cbar.set_label(\"Normalized Discharge (Z-score)\", fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb1ec3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export spatial correlation results and create summary tables\n",
    "print(\"Creating summary tables and saving results...\")\n",
    "\n",
    "# 1. Create summary table of highest and lowest correlations\n",
    "n_top_pairs = 10  # Show top 10 pairs for each category\n",
    "\n",
    "# Get all correlation pairs with basin names\n",
    "correlation_summary = []\n",
    "basin_names = basin_correlation_matrix.columns\n",
    "\n",
    "for i in range(len(basin_names)):\n",
    "    for j in range(i + 1, len(basin_names)):\n",
    "        basin_1, basin_2 = basin_names[i], basin_names[j]\n",
    "        correlation = basin_correlation_matrix.loc[basin_1, basin_2]\n",
    "\n",
    "        if not np.isnan(correlation):\n",
    "            correlation_summary.append(\n",
    "                {\n",
    "                    \"Basin_1\": basin_1,\n",
    "                    \"Basin_2\": basin_2,\n",
    "                    \"Pearson_Correlation\": correlation,\n",
    "                    \"Spearman_Correlation\": basin_correlation_spearman.loc[\n",
    "                        basin_1, basin_2\n",
    "                    ],\n",
    "                    \"Correlation_Strength\": \"Strong\"\n",
    "                    if abs(correlation) > 0.7\n",
    "                    else \"Moderate\"\n",
    "                    if abs(correlation) > 0.3\n",
    "                    else \"Weak\",\n",
    "                    \"Correlation_Type\": \"Positive\" if correlation > 0 else \"Negative\",\n",
    "                }\n",
    "            )\n",
    "\n",
    "correlation_df = pd.DataFrame(correlation_summary)\n",
    "\n",
    "# Sort by correlation strength\n",
    "correlation_df_sorted = correlation_df.sort_values(\n",
    "    \"Pearson_Correlation\", ascending=False\n",
    ")\n",
    "\n",
    "print(f\"\\nTop {n_top_pairs} Most Positively Correlated Basin Pairs:\")\n",
    "print(\"=\" * 70)\n",
    "top_positive = correlation_df_sorted.head(n_top_pairs)\n",
    "for _, row in top_positive.iterrows():\n",
    "    print(\n",
    "        f\"  Basins {int(row['Basin_1'])} - {int(row['Basin_2'])}: r = {row['Pearson_Correlation']:.3f} ({row['Correlation_Strength']})\"\n",
    "    )\n",
    "\n",
    "print(f\"\\nTop {n_top_pairs} Most Negatively Correlated Basin Pairs:\")\n",
    "print(\"=\" * 70)\n",
    "top_negative = correlation_df_sorted.tail(n_top_pairs)\n",
    "for _, row in top_negative.iterrows():\n",
    "    print(\n",
    "        f\"  Basins {int(row['Basin_1'])} - {int(row['Basin_2'])}: r = {row['Pearson_Correlation']:.3f} ({row['Correlation_Strength']})\"\n",
    "    )\n",
    "\n",
    "# 2. Save detailed results to CSV files\n",
    "print(f\"\\nSaving results to CSV files...\")\n",
    "\n",
    "# Save correlation matrix\n",
    "basin_correlation_matrix.to_csv(\"basin_spatial_correlation_matrix.csv\")\n",
    "print(\"  - Basin correlation matrix saved to 'basin_spatial_correlation_matrix.csv'\")\n",
    "\n",
    "# Save pairwise correlation summary\n",
    "correlation_df_sorted.to_csv(\"basin_pairwise_correlations.csv\", index=False)\n",
    "print(\"  - Pairwise correlations saved to 'basin_pairwise_correlations.csv'\")\n",
    "\n",
    "# Save cluster assignments\n",
    "cluster_df = pd.DataFrame(\n",
    "    [\n",
    "        {\"Basin_Code\": basin, \"Cluster\": cluster}\n",
    "        for basin, cluster in basin_to_cluster.items()\n",
    "    ]\n",
    ")\n",
    "cluster_df = cluster_df.sort_values(\"Cluster\")\n",
    "cluster_df.to_csv(\"basin_cluster_assignments.csv\", index=False)\n",
    "print(\"  - Cluster assignments saved to 'basin_cluster_assignments.csv'\")\n",
    "\n",
    "# Save temporal analysis results\n",
    "temporal_df = pd.DataFrame(\n",
    "    {\n",
    "        \"Year\": annual_avg_discharge.index,\n",
    "        \"Average_Discharge\": annual_avg_discharge.values,\n",
    "        \"Std_Discharge\": annual_std_discharge.values,\n",
    "        \"Coefficient_Variation\": annual_cv.values,\n",
    "        \"Discharge_Category\": [\n",
    "            \"High\"\n",
    "            if year in high_discharge_years\n",
    "            else \"Low\"\n",
    "            if year in low_discharge_years\n",
    "            else \"Normal\"\n",
    "            for year in annual_avg_discharge.index\n",
    "        ],\n",
    "        \"Synchronization\": [\n",
    "            \"High\"\n",
    "            if year in synchronized_years\n",
    "            else \"Low\"\n",
    "            if year in desynchronized_years\n",
    "            else \"Medium\"\n",
    "            for year in annual_avg_discharge.index\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "temporal_df.to_csv(\"basin_temporal_synchronization.csv\", index=False)\n",
    "print(\n",
    "    \"  - Temporal synchronization results saved to 'basin_temporal_synchronization.csv'\"\n",
    ")\n",
    "\n",
    "# 3. Print final summary statistics\n",
    "print(f\"\\n\" + \"=\" * 70)\n",
    "print(\"SPATIAL CORRELATION ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Total basins analyzed: {len(basin_correlation_matrix)}\")\n",
    "print(f\"Total basin pairs: {len(correlation_df)}\")\n",
    "print(\n",
    "    f\"Data coverage period: {discharge_matrix.index.min()} - {discharge_matrix.index.max()}\"\n",
    ")\n",
    "\n",
    "print(f\"\\nCorrelation Distribution:\")\n",
    "print(\n",
    "    f\"  Strong positive correlations (>0.7): {len(correlation_df[correlation_df['Pearson_Correlation'] > 0.7])}\"\n",
    ")\n",
    "print(\n",
    "    f\"  Moderate positive correlations (0.3-0.7): {len(correlation_df[(correlation_df['Pearson_Correlation'] > 0.3) & (correlation_df['Pearson_Correlation'] <= 0.7)])}\"\n",
    ")\n",
    "print(\n",
    "    f\"  Weak correlations (-0.3 to 0.3): {len(correlation_df[(correlation_df['Pearson_Correlation'] >= -0.3) & (correlation_df['Pearson_Correlation'] <= 0.3)])}\"\n",
    ")\n",
    "print(\n",
    "    f\"  Negative correlations (<-0.3): {len(correlation_df[correlation_df['Pearson_Correlation'] < -0.3])}\"\n",
    ")\n",
    "\n",
    "print(f\"\\nTemporal Synchronization:\")\n",
    "print(\n",
    "    f\"  High discharge years: {len(high_discharge_years)} ({', '.join(map(str, high_discharge_years))})\"\n",
    ")\n",
    "print(\n",
    "    f\"  Low discharge years: {len(low_discharge_years)} ({', '.join(map(str, low_discharge_years))})\"\n",
    ")\n",
    "print(f\"  Highly synchronized years: {len(synchronized_years)}\")\n",
    "print(f\"  Poorly synchronized years: {len(desynchronized_years)}\")\n",
    "\n",
    "print(f\"\\nCluster Analysis:\")\n",
    "for cluster_id in range(1, n_clusters + 1):\n",
    "    basins_in_cluster = len(\n",
    "        [basin for basin, cluster in basin_to_cluster.items() if cluster == cluster_id]\n",
    "    )\n",
    "    print(f\"  Cluster {cluster_id}: {basins_in_cluster} basins\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2291a305",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "monthly-forecasting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
