{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta-Learner Weights Visualization\n",
    "\n",
    "This notebook visualizes the weights generated by the Historical Performance-Weighted Meta-Learning Framework for the Snow_HistMeta model.\n",
    "\n",
    "## Overview\n",
    "The meta-learner assigns weights to different base models based on their historical performance for each basin-period combination. This notebook explores these weights to understand:\n",
    "- Which models perform best in different periods\n",
    "- How weights vary across basins\n",
    "- Seasonal patterns in model performance\n",
    "- Overall ensemble composition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use(\"seaborn-v0_8\")\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 8)\n",
    "plt.rcParams[\"font.size\"] = 12\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Meta-Learner Weights and Performance Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths to the meta-learner data\n",
    "weights_path = \"../../monthly_forecasting_models/SnowMapper_Based/Snow_HistMeta/Snow_HistMeta_weights.parquet\"\n",
    "performance_path = \"../../monthly_forecasting_models/SnowMapper_Based/Snow_HistMeta/Snow_HistMeta_performance.parquet\"\n",
    "\n",
    "# Load the weights and performance data\n",
    "try:\n",
    "    weights_df = pd.read_parquet(weights_path)\n",
    "    performance_df = pd.read_parquet(performance_path)\n",
    "\n",
    "    print(f\"‚úì Loaded weights data: {weights_df.shape}\")\n",
    "    print(f\"‚úì Loaded performance data: {performance_df.shape}\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"‚ùå Error loading data: {e}\")\n",
    "    print(\n",
    "        \"Please ensure the Snow_HistMeta model has been trained and weights are available.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the structure of the data\n",
    "print(\"=== WEIGHTS DATA STRUCTURE ===\")\n",
    "print(f\"Columns: {list(weights_df.columns)}\")\n",
    "print(f\"Shape: {weights_df.shape}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "display(weights_df.head())\n",
    "\n",
    "print(\"\\n=== PERFORMANCE DATA STRUCTURE ===\")\n",
    "print(f\"Columns: {list(performance_df.columns)}\")\n",
    "print(f\"Shape: {performance_df.shape}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "display(performance_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify model columns (exclude 'code' and 'period')\n",
    "model_columns = [col for col in weights_df.columns if col not in [\"code\", \"period\"]]\n",
    "print(f\"Base models identified: {model_columns}\")\n",
    "print(f\"Number of base models: {len(model_columns)}\")\n",
    "\n",
    "# Basic statistics\n",
    "print(f\"\\nNumber of basins: {weights_df['code'].nunique()}\")\n",
    "print(f\"Number of periods: {weights_df['period'].nunique()}\")\n",
    "print(f\"Period range: {weights_df['period'].min()} to {weights_df['period'].max()}\")\n",
    "print(f\"Basin codes: {sorted(weights_df['code'].unique())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Overall Weight Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate overall weight statistics across all basins and periods\n",
    "weight_stats = weights_df[model_columns].describe()\n",
    "print(\"=== OVERALL WEIGHT STATISTICS ===\")\n",
    "display(weight_stats)\n",
    "\n",
    "# Calculate mean weights across all combinations\n",
    "mean_weights = weights_df[model_columns].mean()\n",
    "print(f\"\\n=== MEAN WEIGHTS ACROSS ALL BASINS AND PERIODS ===\")\n",
    "for model, weight in mean_weights.sort_values(ascending=False).items():\n",
    "    print(f\"{model}: {weight:.4f} ({weight * 100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot overall weight distribution\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. Box plot of weights by model\n",
    "weights_melted = weights_df.melt(\n",
    "    id_vars=[\"code\", \"period\"],\n",
    "    value_vars=model_columns,\n",
    "    var_name=\"Model\",\n",
    "    value_name=\"Weight\",\n",
    ")\n",
    "\n",
    "sns.boxplot(data=weights_melted, x=\"Model\", y=\"Weight\", ax=axes[0, 0])\n",
    "axes[0, 0].set_title(\"Weight Distribution by Model\")\n",
    "axes[0, 0].tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "# 2. Mean weights bar plot\n",
    "mean_weights.plot(kind=\"bar\", ax=axes[0, 1], color=\"skyblue\")\n",
    "axes[0, 1].set_title(\"Mean Weights by Model\")\n",
    "axes[0, 1].set_ylabel(\"Mean Weight\")\n",
    "axes[0, 1].tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "# 3. Weight correlation heatmap\n",
    "correlation_matrix = weights_df[model_columns].corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", center=0, ax=axes[1, 0])\n",
    "axes[1, 0].set_title(\"Model Weight Correlations\")\n",
    "\n",
    "# 4. Histogram of weight values\n",
    "weights_df[model_columns].hist(bins=30, ax=axes[1, 1], alpha=0.7)\n",
    "axes[1, 1].set_title(\"Distribution of All Weights\")\n",
    "axes[1, 1].set_xlabel(\"Weight Value\")\n",
    "axes[1, 1].set_ylabel(\"Frequency\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Seasonal Patterns in Model Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean weights by period (seasonal analysis)\n",
    "weights_by_period = weights_df.groupby(\"period\")[model_columns].mean()\n",
    "\n",
    "print(\"=== SEASONAL WEIGHT PATTERNS ===\")\n",
    "display(weights_by_period.round(4))\n",
    "\n",
    "# Create a mapping from period to month names\n",
    "month_names = [\n",
    "    \"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\",\n",
    "    \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"\n",
    "]\n",
    "\n",
    "def parse_period_to_month(period_str: str) -> str:\n",
    "    \"\"\"Parse period string to get month name and period description.\"\"\"\n",
    "    try:\n",
    "        # Handle different period formats\n",
    "        if isinstance(period_str, (int, float)):\n",
    "            # Simple numeric periods (1-12)\n",
    "            month_idx = int(period_str) - 1\n",
    "            if 0 <= month_idx < 12:\n",
    "                return f\"{period_str} ({month_names[month_idx]})\"\n",
    "            return str(period_str)\n",
    "        \n",
    "        # Handle string periods like \"1-10\", \"1-20\", \"1-end\"\n",
    "        period_str = str(period_str)\n",
    "        if \"-\" in period_str:\n",
    "            month_part, day_part = period_str.split(\"-\", 1)\n",
    "            try:\n",
    "                month_num = int(month_part)\n",
    "                if 1 <= month_num <= 12:\n",
    "                    month_name = month_names[month_num - 1]\n",
    "                    if day_part == \"end\":\n",
    "                        return f\"{period_str} ({month_name} end)\"\n",
    "                    else:\n",
    "                        return f\"{period_str} ({month_name} {day_part})\"\n",
    "                else:\n",
    "                    return period_str\n",
    "            except ValueError:\n",
    "                return period_str\n",
    "        else:\n",
    "            return period_str\n",
    "    except (ValueError, IndexError, AttributeError):\n",
    "        return str(period_str)\n",
    "\n",
    "# Check period data types and analyze format\n",
    "print(f\"\\nPeriod index type: {type(weights_by_period.index[0])}\")\n",
    "print(f\"Sample periods: {list(weights_by_period.index[:5])}\")\n",
    "print(f\"Total periods: {len(weights_by_period.index)}\")\n",
    "\n",
    "# Create enhanced period labels\n",
    "try:\n",
    "    weights_by_period_display = weights_by_period.copy()\n",
    "    weights_by_period_display.index = [\n",
    "        parse_period_to_month(period) for period in weights_by_period_display.index\n",
    "    ]\n",
    "    \n",
    "    print(\"\\n=== SEASONAL PATTERNS WITH ENHANCED LABELS ===\")\n",
    "    display(weights_by_period_display.round(4))\n",
    "    \n",
    "    # Analyze period structure\n",
    "    periods = list(weights_by_period.index)\n",
    "    unique_months = set()\n",
    "    unique_day_parts = set()\n",
    "    \n",
    "    for period in periods:\n",
    "        if \"-\" in str(period):\n",
    "            month_part, day_part = str(period).split(\"-\", 1)\n",
    "            try:\n",
    "                unique_months.add(int(month_part))\n",
    "                unique_day_parts.add(day_part)\n",
    "            except ValueError:\n",
    "                pass\n",
    "    \n",
    "    print(f\"\\n=== PERIOD STRUCTURE ANALYSIS ===\")\n",
    "    print(f\"Unique months found: {sorted(unique_months)}\")\n",
    "    print(f\"Unique day parts: {sorted(unique_day_parts)}\")\n",
    "    print(f\"Period format appears to be: month-daypart (e.g., '1-10' = January 10th)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error processing periods: {e}\")\n",
    "    print(\"Using original period labels without enhancement\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot seasonal patterns\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Line plot of weights by period\n",
    "for model in model_columns:\n",
    "    axes[0, 0].plot(\n",
    "        weights_by_period.index,\n",
    "        weights_by_period[model],\n",
    "        marker=\"o\",\n",
    "        label=model,\n",
    "        linewidth=2,\n",
    "    )\n",
    "axes[0, 0].set_title(\"Seasonal Weight Patterns\")\n",
    "axes[0, 0].set_xlabel(\"Period\")\n",
    "axes[0, 0].set_ylabel(\"Mean Weight\")\n",
    "axes[0, 0].legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Heatmap of weights by period\n",
    "sns.heatmap(\n",
    "    weights_by_period[model_columns].T, annot=True, cmap=\"viridis\", ax=axes[0, 1]\n",
    ")\n",
    "axes[0, 1].set_title(\"Weight Heatmap by Period\")\n",
    "axes[0, 1].set_xlabel(\"Period\")\n",
    "axes[0, 1].set_ylabel(\"Model\")\n",
    "\n",
    "# 3. Stacked bar plot showing relative importance by period\n",
    "weights_by_period[model_columns].plot(kind=\"bar\", stacked=True, ax=axes[1, 0])\n",
    "axes[1, 0].set_title(\"Stacked Weights by Period\")\n",
    "axes[1, 0].set_xlabel(\"Period\")\n",
    "axes[1, 0].set_ylabel(\"Cumulative Weight\")\n",
    "axes[1, 0].legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "axes[1, 0].tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "# 4. Dominant model by period\n",
    "dominant_model = weights_by_period[model_columns].idxmax(axis=1)\n",
    "dominant_counts = dominant_model.value_counts()\n",
    "dominant_counts.plot(kind=\"bar\", ax=axes[1, 1], color=\"lightcoral\")\n",
    "axes[1, 1].set_title(\"Dominant Model Frequency by Period\")\n",
    "axes[1, 1].set_xlabel(\"Model\")\n",
    "axes[1, 1].set_ylabel(\"Number of Periods Dominated\")\n",
    "axes[1, 1].tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Basin-Specific Weight Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean weights by basin\n",
    "weights_by_basin = weights_df.groupby(\"code\")[model_columns].mean()\n",
    "\n",
    "print(\"=== BASIN-SPECIFIC WEIGHT PATTERNS ===\")\n",
    "display(weights_by_basin.round(4))\n",
    "\n",
    "# Find the dominant model for each basin\n",
    "dominant_by_basin = weights_by_basin.idxmax(axis=1)\n",
    "print(\"\\n=== DOMINANT MODEL BY BASIN ===\")\n",
    "for basin, model in dominant_by_basin.items():\n",
    "    weight = weights_by_basin.loc[basin, model]\n",
    "    print(f\"Basin {basin}: {model} (weight: {weight:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot basin-specific patterns\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Heatmap of weights by basin\n",
    "sns.heatmap(weights_by_basin.T, annot=True, cmap=\"plasma\", ax=axes[0, 0])\n",
    "axes[0, 0].set_title(\"Weight Heatmap by Basin\")\n",
    "axes[0, 0].set_xlabel(\"Basin Code\")\n",
    "axes[0, 0].set_ylabel(\"Model\")\n",
    "\n",
    "# 2. Stacked bar plot by basin\n",
    "weights_by_basin.plot(kind=\"bar\", stacked=True, ax=axes[0, 1])\n",
    "axes[0, 1].set_title(\"Stacked Weights by Basin\")\n",
    "axes[0, 1].set_xlabel(\"Basin Code\")\n",
    "axes[0, 1].set_ylabel(\"Cumulative Weight\")\n",
    "axes[0, 1].legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "axes[0, 1].tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "# 3. Box plot of weights by basin\n",
    "weights_melted_basin = weights_df.melt(\n",
    "    id_vars=[\"code\", \"period\"],\n",
    "    value_vars=model_columns,\n",
    "    var_name=\"Model\",\n",
    "    value_name=\"Weight\",\n",
    ")\n",
    "\n",
    "# Select a subset of basins if too many\n",
    "unique_basins = sorted(weights_df[\"code\"].unique())\n",
    "if len(unique_basins) > 10:\n",
    "    selected_basins = unique_basins[:10]\n",
    "    weights_subset = weights_melted_basin[\n",
    "        weights_melted_basin[\"code\"].isin(selected_basins)\n",
    "    ]\n",
    "    title_suffix = f\" (First 10 of {len(unique_basins)} basins)\"\n",
    "else:\n",
    "    weights_subset = weights_melted_basin\n",
    "    title_suffix = \"\"\n",
    "\n",
    "sns.boxplot(data=weights_subset, x=\"code\", y=\"Weight\", ax=axes[1, 0])\n",
    "axes[1, 0].set_title(f\"Weight Distribution by Basin{title_suffix}\")\n",
    "axes[1, 0].set_xlabel(\"Basin Code\")\n",
    "axes[1, 0].tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "# 4. Dominant model distribution\n",
    "dominant_by_basin.value_counts().plot(kind=\"bar\", ax=axes[1, 1], color=\"lightgreen\")\n",
    "axes[1, 1].set_title(\"Dominant Model Distribution Across Basins\")\n",
    "axes[1, 1].set_xlabel(\"Model\")\n",
    "axes[1, 1].set_ylabel(\"Number of Basins Dominated\")\n",
    "axes[1, 1].tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Performance vs. Weight Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge weights and performance data for analysis\n",
    "if \"performance_df\" in locals() and len(performance_df) > 0:\n",
    "    # Merge on code and period\n",
    "    merged_df = pd.merge(\n",
    "        weights_df, performance_df, on=[\"code\", \"period\"], suffixes=(\"_weight\", \"_perf\")\n",
    "    )\n",
    "\n",
    "    print(f\"‚úì Merged weights and performance data: {merged_df.shape}\")\n",
    "    print(f\"Columns: {list(merged_df.columns)}\")\n",
    "\n",
    "    # Get model columns for both weights and performance\n",
    "    weight_cols = [col for col in merged_df.columns if col.endswith(\"_weight\")]\n",
    "    perf_cols = [col for col in merged_df.columns if col.endswith(\"_perf\")]\n",
    "\n",
    "    print(f\"\\nWeight columns: {weight_cols}\")\n",
    "    print(f\"Performance columns: {perf_cols}\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå Performance data not available for comparison\")\n",
    "    merged_df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance vs Weight scatter plots\n",
    "if merged_df is not None:\n",
    "    # Create a mapping between weight and performance columns\n",
    "    model_pairs = []\n",
    "    for model in model_columns:\n",
    "        weight_col = (\n",
    "            f\"{model}_weight\" if f\"{model}_weight\" in merged_df.columns else model\n",
    "        )\n",
    "        perf_col = f\"{model}_perf\" if f\"{model}_perf\" in merged_df.columns else model\n",
    "\n",
    "        if weight_col in merged_df.columns and perf_col in merged_df.columns:\n",
    "            model_pairs.append((model, weight_col, perf_col))\n",
    "\n",
    "    if model_pairs:\n",
    "        n_models = len(model_pairs)\n",
    "        n_cols = min(3, n_models)\n",
    "        n_rows = (n_models + n_cols - 1) // n_cols\n",
    "\n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5 * n_rows))\n",
    "        if n_rows == 1:\n",
    "            axes = axes.reshape(1, -1)\n",
    "\n",
    "        for i, (model, weight_col, perf_col) in enumerate(model_pairs):\n",
    "            row = i // n_cols\n",
    "            col = i % n_cols\n",
    "\n",
    "            # Clean data (remove NaN values)\n",
    "            clean_data = merged_df[[weight_col, perf_col]].dropna()\n",
    "\n",
    "            if len(clean_data) > 0:\n",
    "                axes[row, col].scatter(\n",
    "                    clean_data[perf_col], clean_data[weight_col], alpha=0.6, s=50\n",
    "                )\n",
    "                axes[row, col].set_xlabel(f\"{model} Performance\")\n",
    "                axes[row, col].set_ylabel(f\"{model} Weight\")\n",
    "                axes[row, col].set_title(f\"{model}: Performance vs Weight\")\n",
    "                axes[row, col].grid(True, alpha=0.3)\n",
    "\n",
    "                # Add correlation coefficient\n",
    "                corr = clean_data[perf_col].corr(clean_data[weight_col])\n",
    "                axes[row, col].text(\n",
    "                    0.05,\n",
    "                    0.95,\n",
    "                    f\"r = {corr:.3f}\",\n",
    "                    transform=axes[row, col].transAxes,\n",
    "                    bbox=dict(boxstyle=\"round\", facecolor=\"white\", alpha=0.8),\n",
    "                )\n",
    "            else:\n",
    "                axes[row, col].text(\n",
    "                    0.5,\n",
    "                    0.5,\n",
    "                    \"No data available\",\n",
    "                    transform=axes[row, col].transAxes,\n",
    "                    ha=\"center\",\n",
    "                    va=\"center\",\n",
    "                )\n",
    "\n",
    "        # Hide unused subplots\n",
    "        for i in range(len(model_pairs), n_rows * n_cols):\n",
    "            row = i // n_cols\n",
    "            col = i % n_cols\n",
    "            axes[row, col].set_visible(False)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"‚ùå No matching performance and weight columns found\")\n",
    "else:\n",
    "    print(\"‚ùå Cannot create performance vs weight plots - merged data not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Weight Stability Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate weight variability across periods for each basin\n",
    "weight_variability = weights_df.groupby(\"code\")[model_columns].std()\n",
    "\n",
    "print(\"=== WEIGHT VARIABILITY BY BASIN ===\")\n",
    "print(\"(Standard deviation of weights across periods)\")\n",
    "display(weight_variability.round(4))\n",
    "\n",
    "# Calculate overall weight stability\n",
    "mean_variability = weight_variability.mean()\n",
    "print(\"\\n=== AVERAGE WEIGHT VARIABILITY BY MODEL ===\")\n",
    "for model, var in mean_variability.sort_values().items():\n",
    "    print(f\"{model}: {var:.4f} (lower = more stable)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot weight stability\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Heatmap of weight variability by basin\n",
    "sns.heatmap(weight_variability.T, annot=True, cmap=\"Reds\", ax=axes[0, 0])\n",
    "axes[0, 0].set_title(\"Weight Variability by Basin\\n(Standard Deviation)\")\n",
    "axes[0, 0].set_xlabel(\"Basin Code\")\n",
    "axes[0, 0].set_ylabel(\"Model\")\n",
    "\n",
    "# 2. Mean variability by model\n",
    "mean_variability.plot(kind=\"bar\", ax=axes[0, 1], color=\"orange\")\n",
    "axes[0, 1].set_title(\"Average Weight Variability by Model\")\n",
    "axes[0, 1].set_ylabel(\"Standard Deviation\")\n",
    "axes[0, 1].tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "# 3. Distribution of weight variability\n",
    "weight_variability.hist(bins=20, ax=axes[1, 0], alpha=0.7)\n",
    "axes[1, 0].set_title(\"Distribution of Weight Variability\")\n",
    "axes[1, 0].set_xlabel(\"Standard Deviation\")\n",
    "axes[1, 0].set_ylabel(\"Frequency\")\n",
    "\n",
    "# 4. Coefficient of variation (CV) for each model\n",
    "mean_weights_overall = weights_df[model_columns].mean()\n",
    "cv_by_model = (mean_variability / mean_weights_overall) * 100\n",
    "cv_by_model.plot(kind=\"bar\", ax=axes[1, 1], color=\"purple\")\n",
    "axes[1, 1].set_title(\"Coefficient of Variation by Model\\n(CV = std/mean * 100)\")\n",
    "axes[1, 1].set_ylabel(\"Coefficient of Variation (%)\")\n",
    "axes[1, 1].tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary and Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive summary\n",
    "print(\"=\" * 60)\n",
    "print(\"           SNOW_HISTMETA WEIGHTS ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüìä Dataset Overview:\")\n",
    "print(f\"   ‚Ä¢ {len(model_columns)} base models analyzed\")\n",
    "print(f\"   ‚Ä¢ {weights_df['code'].nunique()} basins\")\n",
    "print(f\"   ‚Ä¢ {weights_df['period'].nunique()} periods\")\n",
    "print(f\"   ‚Ä¢ {len(weights_df)} total weight combinations\")\n",
    "\n",
    "print(f\"\\nüèÜ Model Performance Ranking (by mean weight):\")\n",
    "for i, (model, weight) in enumerate(\n",
    "    mean_weights.sort_values(ascending=False).items(), 1\n",
    "):\n",
    "    print(f\"   {i}. {model}: {weight:.4f} ({weight * 100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nüìà Most Stable Models (lowest variability):\")\n",
    "for i, (model, var) in enumerate(mean_variability.sort_values().items(), 1):\n",
    "    print(f\"   {i}. {model}: œÉ = {var:.4f}\")\n",
    "\n",
    "print(f\"\\nüóìÔ∏è Seasonal Insights:\")\n",
    "# Find periods where each model dominates\n",
    "dominant_periods = weights_by_period[model_columns].idxmax(axis=1)\n",
    "for model in model_columns:\n",
    "    periods = dominant_periods[dominant_periods == model].index.tolist()\n",
    "    if periods:\n",
    "        print(f\"   ‚Ä¢ {model} dominates in periods: {periods}\")\n",
    "\n",
    "print(f\"\\nüèûÔ∏è Basin-Specific Insights:\")\n",
    "dominant_model_counts = dominant_by_basin.value_counts()\n",
    "for model, count in dominant_model_counts.items():\n",
    "    percentage = (count / len(dominant_by_basin)) * 100\n",
    "    print(f\"   ‚Ä¢ {model} dominates {count} basins ({percentage:.1f}%)\")\n",
    "\n",
    "print(f\"\\nüìä Weight Distribution:\")\n",
    "total_weight_check = weights_df[model_columns].sum(axis=1)\n",
    "print(\n",
    "    f\"   ‚Ä¢ Weight sum check: {total_weight_check.mean():.4f} ¬± {total_weight_check.std():.4f}\"\n",
    ")\n",
    "print(f\"   ‚Ä¢ Min weight across all models: {weights_df[model_columns].min().min():.4f}\")\n",
    "print(f\"   ‚Ä¢ Max weight across all models: {weights_df[model_columns].max().max():.4f}\")\n",
    "\n",
    "print(f\"\\nüí° Key Findings:\")\n",
    "best_model = mean_weights.idxmax()\n",
    "best_weight = mean_weights.max()\n",
    "most_stable = mean_variability.idxmin()\n",
    "most_stable_var = mean_variability.min()\n",
    "\n",
    "print(f\"   ‚Ä¢ Best overall model: {best_model} (weight: {best_weight:.4f})\")\n",
    "print(f\"   ‚Ä¢ Most stable model: {most_stable} (variability: {most_stable_var:.4f})\")\n",
    "print(\n",
    "    f\"   ‚Ä¢ Weight balance: {'Well-balanced' if best_weight < 0.5 else 'Dominated by one model'}\"\n",
    ")\n",
    "\n",
    "if merged_df is not None:\n",
    "    print(f\"\\nüîó Performance-Weight Correlation:\")\n",
    "    # Calculate correlations if performance data is available\n",
    "    for model in model_columns:\n",
    "        weight_col = (\n",
    "            f\"{model}_weight\" if f\"{model}_weight\" in merged_df.columns else model\n",
    "        )\n",
    "        perf_col = f\"{model}_perf\" if f\"{model}_perf\" in merged_df.columns else model\n",
    "\n",
    "        if weight_col in merged_df.columns and perf_col in merged_df.columns:\n",
    "            clean_data = merged_df[[weight_col, perf_col]].dropna()\n",
    "            if len(clean_data) > 0:\n",
    "                corr = clean_data[perf_col].corr(clean_data[weight_col])\n",
    "                print(f\"   ‚Ä¢ {model}: r = {corr:.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory for results\n",
    "output_dir = Path(\"../analysis_output\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Export summary statistics\n",
    "summary_stats = pd.DataFrame(\n",
    "    {\n",
    "        \"mean_weight\": mean_weights,\n",
    "        \"weight_variability\": mean_variability,\n",
    "        \"coefficient_of_variation\": cv_by_model,\n",
    "        \"dominant_basins\": dominant_by_basin.value_counts().reindex(\n",
    "            model_columns, fill_value=0\n",
    "        ),\n",
    "    }\n",
    ")\n",
    "\n",
    "summary_stats.to_csv(output_dir / \"snow_histmeta_weight_summary.csv\")\n",
    "\n",
    "# Export detailed weight analysis\n",
    "weights_by_period.to_csv(output_dir / \"snow_histmeta_weights_by_period.csv\")\n",
    "weights_by_basin.to_csv(output_dir / \"snow_histmeta_weights_by_basin.csv\")\n",
    "\n",
    "print(f\"‚úì Results exported to {output_dir}\")\n",
    "print(f\"Files created:\")\n",
    "print(f\"  ‚Ä¢ snow_histmeta_weight_summary.csv\")\n",
    "print(f\"  ‚Ä¢ snow_histmeta_weights_by_period.csv\")\n",
    "print(f\"  ‚Ä¢ snow_histmeta_weights_by_basin.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "This analysis provides comprehensive insights into how the Historical Performance-Weighted Meta-Learning Framework assigns weights to different base models in the Snow_HistMeta ensemble. Key takeaways include:\n",
    "\n",
    "1. **Model Importance**: The ranking of models by average weight across all basins and periods\n",
    "2. **Seasonal Patterns**: How model preferences change throughout the year\n",
    "3. **Basin Specificity**: Which models work best for different basins\n",
    "4. **Weight Stability**: How consistent the weights are across different conditions\n",
    "5. **Performance Correlation**: The relationship between historical performance and assigned weights\n",
    "\n",
    "These insights can be used to:\n",
    "- Understand the meta-learner's decision-making process\n",
    "- Identify potential improvements to base models\n",
    "- Validate the meta-learning approach\n",
    "- Guide future model development"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "monthly-forecasting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
