#!/usr/bin/env python3
"""
Comprehensive test script for SciRegressor model including calibration, hindcasting, operational forecasting, and hyperparameter tuning.

This script tests the complete SciRegressor workflow using synthetic data including:
- Model initialization and configuration loading
- Feature extraction and preprocessing
- Model calibration and hindcasting with Leave-One-Year-Out cross-validation
- Operational forecasting
- Hyperparameter tuning
- Model saving and loading

Usage:
    # Run with pytest
    pytest test_sciregressor.py -v
    
    # Run specific test
    pytest test_sciregressor.py::test_sciregressor_calibration_hindcast -v
    
    # Run with coverage
    pytest test_sciregressor.py --cov=forecast_models.SciRegressor
    
    # Run standalone
    python test_sciregressor.py
    
    # With verbose logging
    python test_sciregressor.py --verbose
    
    # Only show failures
    python test_sciregressor.py --only-failures
"""

import os
import sys
import pandas as pd
import numpy as np
import tempfile
import shutil
import json
from pathlib import Path
from typing import Dict, Any, List
import logging
import datetime
import time
from unittest.mock import patch, MagicMock

# Import comprehensive test utilities
from comprehensive_test_configs import (
    MODEL_TYPES, PREPROCESSING_METHODS, WORKFLOW_COMPONENTS,
    get_test_config, TEST_TIMEOUTS, VALIDATION_CRITERIA
)
from comprehensive_test_utils import (
    ComprehensiveTestDataGenerator, ComprehensiveTestValidator,
    ComprehensiveTestEnvironment, create_mock_patches, run_with_timeout,
    calculate_comprehensive_metrics, format_test_results
)

# Setup logging with enhanced configuration
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('test_sciregressor.log', mode='w')
    ]
)
logger = logging.getLogger(__name__)

# Set specific loggers to appropriate levels
logging.getLogger('forecast_models.SciRegressor').setLevel(logging.DEBUG)
logging.getLogger('scr.FeatureExtractor').setLevel(logging.INFO)
logging.getLogger('scr.FeatureProcessingArtifacts').setLevel(logging.INFO)

# Conditional pytest import
try:
    import pytest
    PYTEST_AVAILABLE = True
except ImportError:
    PYTEST_AVAILABLE = False
    pytest = None
    
    # Create mock pytest for type hints when not available
    class MockPytest:
        def fixture(self, *args, **kwargs):
            def decorator(func):
                return func
            return decorator
    
    if not PYTEST_AVAILABLE:
        pytest = MockPytest()

# Add the parent directory to the path
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

# Import the modules to test
from forecast_models.SciRegressor import SciRegressor
from scr import FeatureExtractor as FE
from eval_scr import metric_functions
import calibrate_hindcast

# Test configurations for SciRegressor
GENERAL_CONFIG = {
    "prediction_horizon": 30,
    "offset": 30,
    "num_features": 5,
    "base_features": ["discharge", "P", "T"],
    "snow_vars": [],
    "forecast_days": ["end"],
    "filter_years": None,
    "model_name": "TestSciRegressor",
    "models": ["xgb", "rf", "catboost"],  # Multiple models for ensemble
    "target": "target",
    "handle_na": "long_term_mean",
    "normalization_type": "standard",
    "normalize_per_basin": False,
    "use_pca": False,
    "use_lr_predictors": False,
    "use_temporal_features": True,
    "use_static_features": True,
    "cat_features": ["code_str"],
    "feature_cols": ["discharge", "P", "T"],
    "static_features": ["area", "elevation"],
    "rivers_to_exclude": [],
    "snow_vars": [],
    "test_years": 3,
    "num_elevation_zones": 5,
    "n_trials": 10
}

FEATURE_CONFIG = {
    "discharge": [
        {
            "operation": "mean",
            "windows": [15, 30],
            "lags": {}
        }
    ],
    "P": [
        {
            "operation": "sum",
            "windows": [30],
            "lags": {}
        }
    ],
    "T": [
        {
            "operation": "mean",
            "windows": [15],
            "lags": {}
        }
    ]
}

MODEL_CONFIG = {
    "xgb": {
        "n_estimators": 10,
        "max_depth": 2,
        "learning_rate": 0.1
    },
    "rf": {
        "n_estimators": 5,
        "max_depth": 2
    },
    "catboost": {
        "iterations": 20,
        "depth": 2,
        "learning_rate": 0.1
    }
}

PATH_CONFIG = {
    "path_discharge": "/fake/path/discharge",
    "path_forcing": "/fake/path/forcing",
    "path_static_data": "/fake/path/static",
    "path_to_sca": "/fake/path/sca",
    "path_to_swe": "/fake/path/swe",
    "path_to_hs": "/fake/path/hs",
    "path_to_rof": "/fake/path/rof",
    "HRU_SWE": "/fake/path/hru_swe",
    "HRU_HS": "/fake/path/hru_hs",
    "HRU_ROF": "/fake/path/hru_rof",
    "path_to_hru_shp": None,
    "model_home_path" : "tests_output"
}

DATA_CONFIG = {
    "start_year": 2000,
    "end_year": 2020,
    "basins": [16936, 16940, 16942]
}

class TestDataGenerator:
    """Helper class to generate synthetic data for testing SciRegressor."""
    
    @staticmethod
    def generate_synthetic_timeseries_data(
        start_date: str = "2000-01-01",
        basin_codes: List[int] = [16936, 16940, 16942],
        noise_level: float = 0.1
    ) -> pd.DataFrame:
        """
        Generate synthetic time series data mimicking hydro-meteorological data.
        Enhanced for SciRegressor testing with more complex relationships.
        
        Args:
            start_date: Start date for data generation
            basin_codes: List of basin codes to generate data for
            noise_level: Amount of random noise to add
            
        Returns:
            DataFrame with synthetic time series data
        """
        end_date = pd.Timestamp.today()
        date_range = pd.date_range(start=start_date, end=end_date, freq='D')
        data_list = []
        
        np.random.seed(42)  # For reproducible tests
        
        for code in basin_codes:
            for date in date_range:
                # Generate seasonal patterns
                day_of_year = date.timetuple().tm_yday
                year = date.year
                
                # Temperature with seasonal cycle and trend
                T = 5 + 15 * np.sin(2 * np.pi * day_of_year / 365.25) + np.random.normal(0, 2)
                T += 0.02 * (year - 2000)  # Small warming trend
                
                # Precipitation with seasonality and extremes
                P = max(0, 10 + 10 * np.sin(2 * np.pi * (day_of_year + 90) / 365.25) + 
                       np.random.exponential(5))
                
                # Add occasional extreme precipitation events
                if np.random.random() < 0.05:  # 5% chance
                    P += np.random.exponential(20)
                
                # Complex discharge model with multiple influences
                base_discharge = 50 + 30 * np.sin(2 * np.pi * (day_of_year + 120) / 365.25)
                
                # Precipitation effect with delay
                precip_effect = P * 0.5
                if len(data_list) > 0:
                    # Add delayed precipitation effect
                    recent_data = [d for d in data_list[-7:] if d['code'] == code]
                    if recent_data:
                        delayed_precip = sum([d['P'] for d in recent_data]) * 0.1
                        precip_effect += delayed_precip
                
                # Temperature effect (snowmelt)
                temp_effect = max(0, T - 0) * 2
                
                # Seasonal baseflow variation
                baseflow_seasonal = 20 * np.sin(2 * np.pi * (day_of_year + 200) / 365.25)
                
                discharge = base_discharge + precip_effect + temp_effect + baseflow_seasonal
                discharge = max(0.1, discharge + np.random.normal(0, 5))
                
                # Add basin-specific scaling and characteristics
                basin_factor = 1 + 0.2 * (code - 16938) / 4
                elevation_effect = (code - 16938) * 0.5  # Higher elevation = cooler
                
                discharge *= basin_factor
                T += elevation_effect
                P *= basin_factor
                
                # Add temporal correlations and noise
                discharge += np.random.normal(0, discharge * noise_level)
                T += np.random.normal(0, 1)
                P = max(0, P + np.random.normal(0, P * noise_level))
                
                data_list.append({
                    'date': date,
                    'code': code,
                    'discharge': round(discharge, 2),
                    'T': round(T, 2),
                    'P': round(P, 2)
                })
        
        df = pd.DataFrame(data_list)
        df = df.sort_values(['code', 'date']).reset_index(drop=True)
        
        logger.info(f"Generated synthetic data: {len(df)} records for {len(basin_codes)} basins")
        logger.info(f"Date range: {df['date'].min()} to {df['date'].max()}")
        logger.info(f"Discharge range: {df['discharge'].min():.2f} to {df['discharge'].max():.2f}")
        logger.info(f"Temperature range: {df['T'].min():.2f} to {df['T'].max():.2f}")
        logger.info(f"Precipitation range: {df['P'].min():.2f} to {df['P'].max():.2f}")
        
        return df
    
    @staticmethod
    def generate_synthetic_static_data(basin_codes: List[int] = [16936, 16940, 16942]) -> pd.DataFrame:
        """
        Generate synthetic static basin characteristics data for SciRegressor.
        
        Args:
            basin_codes: List of basin codes
            
        Returns:
            DataFrame with synthetic static data
        """
        np.random.seed(42)
        
        static_data = []
        for code in basin_codes:
            static_data.append({
                'code': code,
                'area': 100 + np.random.uniform(50, 500),  # kmÂ²
                'area_km2': 100 + np.random.uniform(50, 500),  # kmÂ² (for SciRegressor)
                'elevation': 1000 + np.random.uniform(500, 2000),  # m
                'slope': np.random.uniform(5, 30),  # degrees
                'forest_cover': np.random.uniform(0.1, 0.8),  # fraction
                'latitude': 40 + np.random.uniform(-5, 5),  # degrees
                'longitude': 70 + np.random.uniform(-10, 10),  # degrees
            })
        
        df = pd.DataFrame(static_data)
        logger.info(f"Generated static data for {len(basin_codes)} basins")
        
        return df

class SciRegressorTester:
    """Main test class for SciRegressor model."""
    
    def __init__(self):
        """Initialize the tester with synthetic data and configurations."""
        self.test_dir = None
        self.data = None
        self.static_data = None
        self.configs = None
        self.only_failures = False

class ComprehensiveSciRegressorTester:
    """Comprehensive test class for SciRegressor model with all combinations."""
    
    def __init__(self):
        """Initialize the comprehensive tester."""
        self.test_results = []
        self.failed_tests = []
        self.passed_tests = []
        self.only_failures = False
        
    def _run_single_test(self, test_func, test_name: str, timeout: int = 120) -> bool:
        """Run a single test with timeout and error handling."""
        start_time = time.time()
        
        try:
            if not self.only_failures:
                logger.info(f"Running {test_name}...")
            
            # Run test with timeout
            result = run_with_timeout(test_func, timeout)
            
            execution_time = time.time() - start_time
            
            if not self.only_failures:
                logger.info(f"âœ“ {test_name} passed ({execution_time:.2f}s)")
            else:
                print(f"âœ“ {test_name} passed ({execution_time:.2f}s)")
            
            # Format and store results
            test_result = format_test_results(test_name, True, execution_time=execution_time)
            self.test_results.append(test_result)
            self.passed_tests.append(test_name)
            
            return True
            
        except Exception as e:
            execution_time = time.time() - start_time
            
            # Always show detailed logs for failures
            logger.error(f"âœ— {test_name} failed: {e}")
            logger.error(f"Exception type: {type(e).__name__}")
            logger.error(f"Exception details:", exc_info=True)
            
            # Also print to console for immediate visibility
            print(f"âœ— {test_name} FAILED: {e}")
            print(f"Check log file for detailed traceback")
            
            # Format and store results
            test_result = format_test_results(test_name, False, execution_time=execution_time)
            self.test_results.append(test_result)
            self.failed_tests.append(test_name)
            
            return False
    
    def _create_model_instance(self, model_type: str, preprocessing_method: str):
        """Create a SciRegressor model instance for testing."""
        # Setup test environment
        env = ComprehensiveTestEnvironment(model_type, preprocessing_method)
        env.setup()
        
        try:
            # Create model with mocked dependencies
            with patch.object(SciRegressor, '__preprocess_data__') as mock_preprocess:
                with patch('scr.data_utils.glacier_mapper_features') as mock_glacier:
                    mock_glacier.return_value = env.data
                    mock_preprocess.return_value = None
                    
                    model = SciRegressor(
                        data=env.data,
                        static_data=env.static_data,
                        general_config=env.configs['general_config'],
                        model_config=env.configs['model_config'],
                        feature_config=env.configs['feature_config'],
                        path_config=env.configs['path_config']
                    )
                    
                    return model, env
        except Exception as e:
            env.teardown()
            raise e
    
    def test_hyperparameter_tuning(self, model_type: str, preprocessing_method: str) -> bool:
        """Test hyperparameter tuning for specific model and preprocessing method."""
        model, env = self._create_model_instance(model_type, preprocessing_method)
        
        try:
            # Mock the tuning process for faster testing
            with patch.object(model, 'tune_hyperparameters') as mock_tune:
                mock_tune.return_value = (True, "Hyperparameters tuned successfully")
                
                success, message = model.tune_hyperparameters()
                
                # Validate results
                assert isinstance(success, bool), "Tuning should return boolean success status"
                assert isinstance(message, str), "Tuning should return string message"
                assert success, f"Tuning failed: {message}"
                
                # Validate model initialization
                ComprehensiveTestValidator.validate_model_initialization(
                    model, model_type, preprocessing_method
                )
                
                return True
                
        finally:
            env.teardown()
    
    def test_calibration(self, model_type: str, preprocessing_method: str) -> bool:
        """Test calibration for specific model and preprocessing method."""
        model, env = self._create_model_instance(model_type, preprocessing_method)
        
        try:
            # Mock the calibration process
            with patch.object(model, 'calibrate_model_and_hindcast') as mock_calibrate:
                # Create mock calibration result
                mock_data = env.data.copy()
                mock_data['Q_obs'] = mock_data['discharge']
                mock_data[f'Q_{model.name}'] = mock_data['discharge'] * (1 + np.random.normal(0, 0.1, len(mock_data)))
                mock_calibrate.return_value = mock_data.head(100)  # Return subset for faster testing
                
                calibration_result = model.calibrate_model_and_hindcast()
                
                # Validate results
                assert len(calibration_result) > 0, "Calibration should return results"
                
                ComprehensiveTestValidator.validate_predictions(
                    calibration_result, model.name, "calibration"
                )
                
                return True
                
        finally:
            env.teardown()
    
    def test_hindcast(self, model_type: str, preprocessing_method: str) -> bool:
        """Test hindcast for specific model and preprocessing method."""
        model, env = self._create_model_instance(model_type, preprocessing_method)
        
        try:
            # Mock the hindcast process
            with patch.object(model, 'calibrate_model_and_hindcast') as mock_hindcast:
                # Create mock hindcast result
                mock_data = env.data.copy()
                mock_data['Q_obs'] = mock_data['discharge']
                mock_data[f'Q_{model.name}'] = mock_data['discharge'] * (1 + np.random.normal(0, 0.1, len(mock_data)))
                mock_hindcast.return_value = mock_data.head(100)  # Return subset for faster testing
                
                hindcast_result = model.calibrate_model_and_hindcast()
                
                # Validate results
                assert len(hindcast_result) > 0, "Hindcast should return results"
                
                ComprehensiveTestValidator.validate_predictions(
                    hindcast_result, model.name, "hindcast"
                )
                
                return True
                
        finally:
            env.teardown()
    
    def test_operational_prediction(self, model_type: str, preprocessing_method: str) -> bool:
        """Test operational prediction for specific model and preprocessing method."""
        model, env = self._create_model_instance(model_type, preprocessing_method)
        
        try:
            # Mock the operational prediction process
            with patch.object(model, 'predict_operational') as mock_predict:
                with patch.object(model, 'load_model') as mock_load:
                    # Create mock operational prediction result
                    today = datetime.datetime.now()
                    mock_forecast_data = []
                    for code in [16936, 16940, 16942]:
                        mock_forecast_data.append({
                            'code': code,
                            'forecast_date': today.strftime('%Y-%m-%d'),
                            'valid_from': '2025-07-04',
                            'valid_to': '2025-08-03',
                            f'Q_{model.name}': np.random.uniform(10, 100)
                        })
                    
                    mock_forecast_df = pd.DataFrame(mock_forecast_data)
                    mock_predict.return_value = mock_forecast_df
                    mock_load.return_value = {model_type: {}}
                    
                    operational_result = model.predict_operational()
                    
                    # Validate results
                    if len(operational_result) > 0:
                        ComprehensiveTestValidator.validate_predictions(
                            operational_result, model.name, "operational_prediction"
                        )
                    
                    return True
                    
        finally:
            env.teardown()
    
    def test_complete_workflow(self, model_type: str, preprocessing_method: str) -> bool:
        """Test complete workflow for specific model and preprocessing method."""
        model, env = self._create_model_instance(model_type, preprocessing_method)
        
        try:
            # Test complete workflow: tuning -> calibration -> operational prediction
            with patch.object(model, 'tune_hyperparameters') as mock_tune:
                with patch.object(model, 'calibrate_model_and_hindcast') as mock_calibrate:
                    with patch.object(model, 'predict_operational') as mock_predict:
                        with patch.object(model, 'load_model') as mock_load:
                            # Mock tuning
                            mock_tune.return_value = (True, "Success")
                            
                            # Mock calibration
                            mock_data = env.data.copy()
                            mock_data['Q_obs'] = mock_data['discharge']
                            mock_data[f'Q_{model.name}'] = mock_data['discharge'] * (1 + np.random.normal(0, 0.1, len(mock_data)))
                            mock_calibrate.return_value = mock_data.head(100)
                            
                            # Mock operational prediction
                            today = datetime.datetime.now()
                            mock_forecast_data = []
                            for code in [16936, 16940, 16942]:
                                mock_forecast_data.append({
                                    'code': code,
                                    'forecast_date': today.strftime('%Y-%m-%d'),
                                    'valid_from': '2025-07-04',
                                    'valid_to': '2025-08-03',
                                    f'Q_{model.name}': np.random.uniform(10, 100)
                                })
                            mock_forecast_df = pd.DataFrame(mock_forecast_data)
                            mock_predict.return_value = mock_forecast_df
                            mock_load.return_value = {model_type: {}}
                            
                            # Execute workflow
                            success, message = model.tune_hyperparameters()
                            assert success, f"Tuning failed: {message}"
                            
                            calibration_result = model.calibrate_model_and_hindcast()
                            assert len(calibration_result) > 0, "Calibration should return results"
                            
                            operational_result = model.predict_operational()
                            # Operational result can be empty if no current data
                            
                            return True
                            
        finally:
            env.teardown()
    
    def test_multi_model_ensemble(self, preprocessing_method: str) -> bool:
        """Test multi-model ensemble for specific preprocessing method."""
        models = {}
        envs = []
        
        try:
            # Create multiple models
            for model_type in MODEL_TYPES:
                model, env = self._create_model_instance(model_type, preprocessing_method)
                models[model_type] = model
                envs.append(env)
            
            # Test ensemble functionality
            with patch.object(SciRegressor, 'calibrate_model_and_hindcast') as mock_calibrate:
                # Create mock ensemble result
                mock_data = envs[0].data.copy()
                mock_data['Q_obs'] = mock_data['discharge']
                
                # Add predictions for each model
                for model_type in MODEL_TYPES:
                    mock_data[f'Q_{model_type}'] = mock_data['discharge'] * (1 + np.random.normal(0, 0.1, len(mock_data)))
                
                # Add ensemble prediction
                pred_cols = [f'Q_{model_type}' for model_type in MODEL_TYPES]
                mock_data['Q_ensemble'] = mock_data[pred_cols].mean(axis=1)
                
                mock_calibrate.return_value = mock_data.head(100)
                
                # Test with first model (representative)
                ensemble_result = models[MODEL_TYPES[0]].calibrate_model_and_hindcast()
                
                # Validate ensemble results
                assert len(ensemble_result) > 0, "Ensemble should return results"
                
                return True
                
        finally:
            for env in envs:
                env.teardown()
    
    def run_comprehensive_tests(self) -> bool:
        """Run all comprehensive tests."""
        logger.info("="*80)
        logger.info("STARTING COMPREHENSIVE SCIREGRESSOR TESTS")
        logger.info("="*80)
        
        total_tests = 0
        
        # Phase 1: Individual Component Tests (48 tests)
        component_tests = {
            "hyperparameter_tuning": (self.test_hyperparameter_tuning, TEST_TIMEOUTS["hyperparameter_tuning"]),
            "calibration": (self.test_calibration, TEST_TIMEOUTS["calibration"]),
            "hindcast": (self.test_hindcast, TEST_TIMEOUTS["hindcast"]),
            "operational_prediction": (self.test_operational_prediction, TEST_TIMEOUTS["operational_prediction"])
        }
        
        for component, (test_func, timeout) in component_tests.items():
            for model_type in MODEL_TYPES:
                for preprocessing_method in PREPROCESSING_METHODS:
                    total_tests += 1
                    test_name = f"{model_type}_{component}_{preprocessing_method}"
                    
                    def test_wrapper():
                        return test_func(model_type, preprocessing_method)
                    
                    print(f"\n[{len(self.test_results)+1}/{total_tests}] {test_name}...")
                    self._run_single_test(test_wrapper, test_name, timeout)
        
        # Phase 2: Complete Workflow Tests (12 tests)
        for model_type in MODEL_TYPES:
            for preprocessing_method in PREPROCESSING_METHODS:
                total_tests += 1
                test_name = f"{model_type}_complete_workflow_{preprocessing_method}"
                
                def test_wrapper():
                    return self.test_complete_workflow(model_type, preprocessing_method)
                
                print(f"\n[{len(self.test_results)+1}/{total_tests}] {test_name}...")
                self._run_single_test(test_wrapper, test_name, TEST_TIMEOUTS["complete_workflow"])
        
        # Phase 3: Multi-Model Integration Tests (4 tests)
        for preprocessing_method in PREPROCESSING_METHODS:
            total_tests += 1
            test_name = f"multi_model_ensemble_{preprocessing_method}"
            
            def test_wrapper():
                return self.test_multi_model_ensemble(preprocessing_method)
            
            print(f"\n[{len(self.test_results)+1}/{total_tests}] {test_name}...")
            self._run_single_test(test_wrapper, test_name, TEST_TIMEOUTS["multi_model_ensemble"])
        
        # Summary
        passed_count = len(self.passed_tests)
        failed_count = len(self.failed_tests)
        
        logger.info("="*80)
        logger.info("COMPREHENSIVE TEST SUMMARY")
        logger.info("="*80)
        logger.info(f"Total tests run: {len(self.test_results)}")
        logger.info(f"Tests passed: {passed_count}")
        logger.info(f"Tests failed: {failed_count}")
        
        if self.failed_tests:
            logger.error(f"Failed tests: {', '.join(self.failed_tests)}")
            print(f"\nâŒ FAILED TESTS: {', '.join(self.failed_tests)}")
        
        if passed_count == len(self.test_results):
            logger.info("ðŸŽ‰ ALL COMPREHENSIVE TESTS PASSED!")
            print("ðŸŽ‰ ALL COMPREHENSIVE TESTS PASSED!")
            return True
        else:
            logger.warning(f"âš ï¸  {failed_count} tests failed")
            print(f"âš ï¸  {failed_count} tests failed")
            return False
        
    def setup_test_environment(self):
        """Set up test environment with synthetic data and temporary directories."""
        # Create temporary directory for test outputs
        self.test_dir = tempfile.mkdtemp(prefix="sciregressor_test_")
        logger.info(f"Created test directory: {self.test_dir}")
        
        # Generate synthetic data
        self.data = TestDataGenerator.generate_synthetic_timeseries_data()
        self.static_data = TestDataGenerator.generate_synthetic_static_data()
        
        # Setup configurations
        self.configs = {
            'general_config': GENERAL_CONFIG.copy(),
            'feature_config': FEATURE_CONFIG.copy(),
            'model_config': MODEL_CONFIG.copy(),
            'path_config': PATH_CONFIG.copy(),
            'data_config': DATA_CONFIG.copy()
        }
        
        # Create config files in test directory
        config_dir = Path(self.test_dir) / "config"
        config_dir.mkdir(exist_ok=True)
        
        for config_name, config_data in self.configs.items():
            if config_name != 'path_config':
                config_file = config_dir / f"{config_name}.json"
                with open(config_file, 'w') as f:
                    json.dump(config_data, f, indent=2)
        
        logger.info("Test environment setup complete")
        
    def teardown_test_environment(self):
        """Clean up test environment."""
        if self.test_dir and os.path.exists(self.test_dir):
            shutil.rmtree(self.test_dir)
            logger.info(f"Cleaned up test directory: {self.test_dir}")
    
    def _run_test_with_detailed_logging(self, test_func, test_name):
        """
        Run a test with detailed logging on failure.
        
        Args:
            test_func: The test function to run
            test_name: Name of the test for logging
            
        Returns:
            tuple: (success: bool, result: Any)
        """
        try:
            if not self.only_failures:
                logger.info(f"Running {test_name}...")
            
            result = test_func()
            
            if not self.only_failures:
                logger.info(f"âœ“ {test_name} passed")
            else:
                print(f"âœ“ {test_name} passed")
            
            return True, result
            
        except Exception as e:
            # Always show detailed logs for failures
            logger.error(f"âœ— {test_name} failed: {e}")
            logger.error(f"Exception type: {type(e).__name__}")
            logger.error(f"Exception details:", exc_info=True)
            
            # Also print to console for immediate visibility
            print(f"âœ— {test_name} FAILED: {e}")
            print(f"Check log file for detailed traceback")
            
            return False, None
    
    def test_model_initialization(self):
        """Test SciRegressor model initialization."""
        logger.info("Testing SciRegressor model initialization...")
        
        try:
            # Mock the preprocessing methods that might fail due to missing dependencies
            with patch.object(SciRegressor, '__preprocess_data__') as mock_preprocess:
                mock_preprocess.return_value = None
                
                model = SciRegressor(
                    data=self.data,
                    static_data=self.static_data,
                    general_config=self.configs['general_config'],
                    model_config=self.configs['model_config'],
                    feature_config=self.configs['feature_config'],
                    path_config=self.configs['path_config']
                )
            
            # Verify model attributes
            assert model.name == "TestSciRegressor"
            assert hasattr(model, 'data')
            assert hasattr(model, 'static_data')
            assert hasattr(model, 'general_config')
            assert hasattr(model, 'model_config')
            assert hasattr(model, 'feature_config')
            assert hasattr(model, 'models')
            assert hasattr(model, 'fitted_models')
            
            # Check model-specific attributes

            expected_models = MODEL_CONFIG.keys()
            expected_models = list(expected_models)  # Convert to list for comparison
            assert model.models == expected_models
            assert model.target == "target"
            assert model.feature_cols == ["discharge", "P", "T"]
            assert model.static_features == ["area", "elevation"]
            
            logger.info("âœ“ SciRegressor model initialization test passed")
            return True
            
        except Exception as e:
            logger.error(f"âœ— SciRegressor model initialization test failed: {e}")
            raise
    
    def test_feature_extraction(self):
        """Test feature extraction functionality."""
        logger.info("Testing feature extraction...")
        
        try:
            # Mock external dependencies that might not be available
            with patch.object(SciRegressor, '__preprocess_data__') as mock_preprocess:
                with patch('scr.data_utils.glacier_mapper_features') as mock_glacier:
                    mock_glacier.return_value = self.data
                    mock_preprocess.return_value = None
                    
                    model = SciRegressor(
                        data=self.data,
                        static_data=self.static_data,
                        general_config=self.configs['general_config'],
                        model_config=self.configs['model_config'],
                        feature_config=self.configs['feature_config'],
                        path_config=self.configs['path_config']
                    )
                    
                    # Call feature extraction manually
                    model.__extract_features__()
            
            # Check that features were extracted
            assert 'year' in model.data.columns
            assert 'month' in model.data.columns
            assert 'month_sin' in model.data.columns
            assert 'month_cos' in model.data.columns
            assert 'week_sin' in model.data.columns
            assert 'week_cos' in model.data.columns
            
            # Check for feature columns
            feature_cols = [col for col in model.data.columns if any(f in col for f in ['discharge', 'P_', 'T_'])]
            assert len(feature_cols) > 0, "No feature columns found"
            
            logger.info("âœ“ Feature extraction test passed")
            return True
            
        except Exception as e:
            logger.error(f"âœ— Feature extraction test failed: {e}")
            raise
    
    def test_calibration_and_hindcast(self):
        """Test model calibration and hindcasting."""
        logger.info("Testing calibration and hindcasting...")
        
        try:           
                        
            model = SciRegressor(
                data=self.data,
                static_data=self.static_data,
                general_config=self.configs['general_config'],
                model_config=self.configs['model_config'],
                feature_config=self.configs['feature_config'],
                path_config=self.configs['path_config']
            )
                        
            # Run calibration and hindcasting
            hindcast_df = model.calibrate_model_and_hindcast()

            # Verify results
            assert len(hindcast_df) > 0, "No hindcast predictions generated"
            
            # Check required columns
            required_cols = ['date', 'code', 'Q_obs', f'Q_{model.name}']
            for col in required_cols:
                assert col in hindcast_df.columns, f"Missing required column: {col}"
            
            # Verify predictions for each basin
            unique_basins = hindcast_df['code'].unique()
            assert len(unique_basins) >= 1, "No basins in hindcast results"
            
            # Check that predictions are reasonable
            pred_col = f'Q_{model.name}'
            assert not hindcast_df[pred_col].isna().all(), "All predictions are NaN"
            
            logger.info(f"âœ“ Calibration and hindcast test passed")
            logger.info(f"  Generated {len(hindcast_df)} predictions for {len(unique_basins)} basins")
            
            return hindcast_df
            
        except Exception as e:
            logger.error(f"âœ— Calibration and hindcast test failed: {e}")
            raise
    
    def test_operational_forecast(self):
        """Test operational forecasting functionality."""
        logger.info("Testing operational forecasting...")
        
        try:
            # Mock external dependencies
            with patch.object(SciRegressor, '__preprocess_data__') as mock_preprocess:
                with patch('scr.data_utils.glacier_mapper_features') as mock_glacier:
                    with patch.object(SciRegressor, 'load_model') as mock_load:
                        with patch.object(SciRegressor, 'predict_operational') as mock_predict:
                            # Create a mock forecast result
                            mock_forecast_data = []
                            for code in [16936, 16940, 16942]:
                                mock_forecast_data.append({
                                    'forecast_date': datetime.datetime.now(),
                                    'code': code,
                                    'valid_from': '2025-07-04',
                                    'valid_to': '2025-08-03',
                                    'Q_TestSciRegressor': np.random.uniform(10, 100)
                                })
                            
                            mock_forecast_df = pd.DataFrame(mock_forecast_data)
                            mock_predict.return_value = mock_forecast_df
                            mock_load.return_value = {'xgboost': {}, 'random_forest': {}}
                            
                            mock_glacier.return_value = self.data
                            mock_preprocess.return_value = None
                            
                            model = SciRegressor(
                                data=self.data,
                                static_data=self.static_data,
                                general_config=self.configs['general_config'],
                                model_config=self.configs['model_config'],
                                feature_config=self.configs['feature_config'],
                                path_config=self.configs['path_config']
                            )
                            
                            # Run operational forecast
                            forecast_df = model.predict_operational()
            
            # Verify results
            if len(forecast_df) > 0:
                # Check required columns
                required_cols = ['forecast_date', 'code', 'valid_from', 'valid_to', f'Q_{model.name}']
                for col in required_cols:
                    assert col in forecast_df.columns, f"Missing required column: {col}"
                
                # Check that predictions are reasonable
                pred_col = f'Q_{model.name}'
                assert not forecast_df[pred_col].isna().all(), "All forecasts are NaN"
                
                # Check date consistency
                assert all(pd.to_datetime(forecast_df['valid_from']) < pd.to_datetime(forecast_df['valid_to']))
                
                logger.info(f"âœ“ Operational forecast test passed")
                logger.info(f"  Generated forecasts for {len(forecast_df)} basins")
            else:
                logger.info("âœ“ Operational forecast test passed (no forecasts for test period)")
            
            return forecast_df
            
        except Exception as e:
            logger.error(f"âœ— Operational forecast test failed: {e}")
            raise
    
    def test_hyperparameter_tuning(self):
        """Test hyperparameter tuning functionality."""
        logger.info("Testing hyperparameter tuning...")
        
        try:
            # Mock external dependencies
            with patch.object(SciRegressor, '__preprocess_data__') as mock_preprocess:
                with patch('scr.data_utils.glacier_mapper_features') as mock_glacier:
                    with patch.object(SciRegressor, 'tune_hyperparameters') as mock_tune:
                        # Mock the tuning result
                        mock_tune.return_value = (True, "Hyperparameters tuned successfully")
                        
                        mock_glacier.return_value = self.data
                        mock_preprocess.return_value = None
                        
                        model = SciRegressor(
                            data=self.data,
                            static_data=self.static_data,
                            general_config=self.configs['general_config'],
                            model_config=self.configs['model_config'],
                            feature_config=self.configs['feature_config'],
                            path_config=self.configs['path_config']
                        )
                        
                        # Test hyperparameter tuning
                        success, message = model.tune_hyperparameters()
            
            # Verify results
            assert isinstance(success, bool), "Tuning should return boolean success status"
            assert isinstance(message, str), "Tuning should return string message"
            
            logger.info(f"âœ“ Hyperparameter tuning test passed")
            logger.info(f"  Tuning result: {success}, Message: {message}")
            
            return success, message
            
        except Exception as e:
            logger.error(f"âœ— Hyperparameter tuning test failed: {e}")
            raise
    
    def test_model_persistence(self):
        """Test model saving and loading functionality."""
        logger.info("Testing model persistence...")
        
        try:
            # Mock external dependencies
            with patch.object(SciRegressor, '__preprocess_data__') as mock_preprocess:
                with patch('scr.data_utils.glacier_mapper_features') as mock_glacier:
                    with patch.object(SciRegressor, 'save_model') as mock_save:
                        with patch.object(SciRegressor, 'load_model') as mock_load:
                            mock_save.return_value = None
                            mock_load.return_value = {'xgboost': {}, 'random_forest': {}}
                            
                            mock_glacier.return_value = self.data
                            mock_preprocess.return_value = None
                            
                            model = SciRegressor(
                                data=self.data,
                                static_data=self.static_data,
                                general_config=self.configs['general_config'],
                                model_config=self.configs['model_config'],
                                feature_config=self.configs['feature_config'],
                                path_config=self.configs['path_config']
                            )
                            
                            # Test saving
                            model.save_model()
                            logger.info("âœ“ Model saving test passed")
                            
                            # Test loading
                            loaded_models = model.load_model()
                            logger.info("âœ“ Model loading test passed")
            
            return True
            
        except Exception as e:
            logger.error(f"âœ— Model persistence test failed: {e}")
            raise
    
    def test_configuration_loading(self):
        """Test configuration loading functionality."""
        logger.info("Testing configuration loading...")
        
        try:
            config_dir = Path(self.test_dir) / "config"
            
            # Test the load_configuration function from calibrate_hindcast
            configs = calibrate_hindcast.load_configuration(str(config_dir))
            
            # Verify all expected configs are loaded
            expected_configs = ['general_config', 'model_config', 'feature_config', 'data_config']
            for config_name in expected_configs:
                assert config_name in configs, f"Missing config: {config_name}"
                assert len(configs[config_name]) > 0, f"Empty config: {config_name}"
            
            logger.info("âœ“ Configuration loading test passed")
            return True
            
        except Exception as e:
            logger.error(f"âœ— Configuration loading test failed: {e}")
            raise
    
    def test_metrics_calculation(self, hindcast_df: pd.DataFrame):
        """Test metrics calculation on hindcast results."""
        logger.info("Testing metrics calculation...")
        
        try:
            if len(hindcast_df) == 0:
                logger.info("âœ“ Metrics calculation test skipped (no hindcast data)")
                return pd.DataFrame()
            
            # Calculate metrics using the same logic as in calibrate_hindcast.py
            metrics_list = []
            model_name = "TestSciRegressor"
            prediction_col = f"Q_{model_name}"
            
            for code, group in hindcast_df.groupby(['code']):
                if len(group) < 5:  # Need sufficient data points
                    continue
                
                obs = group['Q_obs'].values
                pred = group[prediction_col].values
                
                # Calculate various metrics
                try:
                    metrics = {
                        'code': code,
                        'n_predictions': len(group),
                        'r2': metric_functions.r2_score(obs, pred),
                        'rmse': metric_functions.rmse(obs, pred),
                        'mae': metric_functions.mae(obs, pred),
                        'bias': metric_functions.bias(obs, pred),
                    }
                    
                    # Add NSE if available
                    if hasattr(metric_functions, 'nse'):
                        metrics['nse'] = metric_functions.nse(obs, pred)
                    
                    metrics_list.append(metrics)
                    
                except Exception as e:
                    logger.warning(f"Failed to calculate metrics for basin {code}: {e}")
            
            if not metrics_list:
                logger.info("âœ“ Metrics calculation test passed (no valid metrics)")
                return pd.DataFrame()
            
            metrics_df = pd.DataFrame(metrics_list)
            
            # Verify metrics are reasonable
            assert len(metrics_df) > 0, "No metrics calculated"
            assert 'r2' in metrics_df.columns, "R2 metric missing"
            assert 'rmse' in metrics_df.columns, "RMSE metric missing"
            assert 'mae' in metrics_df.columns, "MAE metric missing"
            
            # Calculate summary statistics
            summary_stats = {
                'r2_mean': metrics_df['r2'].mean(),
                'r2_median': metrics_df['r2'].median(),
                'rmse_mean': metrics_df['rmse'].mean(),
                'mae_mean': metrics_df['mae'].mean(),
                'n_basins': len(metrics_df)
            }
            
            logger.info("âœ“ Metrics calculation test passed")
            logger.info(f"  Calculated metrics for {len(metrics_df)} basins")
            logger.info(f"  Mean RÂ²: {summary_stats['r2_mean']:.3f}")
            logger.info(f"  Mean RMSE: {summary_stats['rmse_mean']:.3f}")
            logger.info(f"  Mean MAE: {summary_stats['mae_mean']:.3f}")
            
            return metrics_df
            
        except Exception as e:
            logger.error(f"âœ— Metrics calculation test failed: {e}")
            raise
    
    
    def run_all_tests(self):
        """Run all tests in sequence."""
        logger.info("="*60)
        logger.info("STARTING SCIREGRESSOR MODEL TESTS")
        logger.info("="*60)
        
        try:
            # Setup
            self.setup_test_environment()
            
            # Define tests with their names
            test_suite = [
                (self.test_model_initialization, "Model Initialization"),
                (self.test_feature_extraction, "Feature Extraction"),
                (self.test_configuration_loading, "Configuration Loading"),
                (self.test_calibration_and_hindcast, "Calibration and Hindcast"),
                (self.test_operational_forecast, "Operational Forecast"),
                (self.test_hyperparameter_tuning, "Hyperparameter Tuning"),
                (self.test_model_persistence, "Model Persistence"),
            ]
            
            passed_tests = 0
            total_tests = len(test_suite)
            failed_tests = []
            
            for i, (test_func, test_name) in enumerate(test_suite, 1):
                print(f"\n[{i}/{total_tests}] {test_name}...")
                
                success, result = self._run_test_with_detailed_logging(test_func, test_name)
                
                if success:
                    passed_tests += 1
                else:
                    failed_tests.append(test_name)
            
            # Summary
            logger.info("="*60)
            logger.info("TEST SUMMARY")
            logger.info("="*60)
            logger.info(f"Tests passed: {passed_tests}/{total_tests}")
            
            if failed_tests:
                logger.error(f"Failed tests: {', '.join(failed_tests)}")
                print(f"\nâŒ FAILED TESTS: {', '.join(failed_tests)}")
            
            if passed_tests == total_tests:
                logger.info("ðŸŽ‰ ALL TESTS PASSED!")
                print("ðŸŽ‰ ALL TESTS PASSED!")
                return True
            else:
                logger.warning(f"âš ï¸  {total_tests - passed_tests} tests failed")
                print(f"âš ï¸  {total_tests - passed_tests} tests failed")
                return False
                
        finally:
            # Cleanup
            self.teardown_test_environment()

# Pytest-compatible test functions
if PYTEST_AVAILABLE:
    @pytest.fixture
    def sciregressor_tester():
        """Pytest fixture for SciRegressorTester."""
        tester = SciRegressorTester()
        tester.setup_test_environment()
        yield tester
        tester.teardown_test_environment()
    
    @pytest.fixture
    def comprehensive_tester():
        """Pytest fixture for ComprehensiveSciRegressorTester."""
        return ComprehensiveSciRegressorTester()
    
    def test_sciregressor_initialization(sciregressor_tester):
        """Pytest: Test model initialization."""
        sciregressor_tester.test_model_initialization()
    
    def test_sciregressor_feature_extraction(sciregressor_tester):
        """Pytest: Test feature extraction."""
        sciregressor_tester.test_feature_extraction()
    
    def test_sciregressor_calibration_hindcast(sciregressor_tester):
        """Pytest: Test calibration and hindcasting."""
        sciregressor_tester.test_calibration_and_hindcast()
    
    def test_sciregressor_operational_forecast(sciregressor_tester):
        """Pytest: Test operational forecasting."""
        sciregressor_tester.test_operational_forecast()
    
    def test_sciregressor_hyperparameter_tuning(sciregressor_tester):
        """Pytest: Test hyperparameter tuning."""
        sciregressor_tester.test_hyperparameter_tuning()
    
    def test_sciregressor_configuration_loading(sciregressor_tester):
        """Pytest: Test configuration loading."""
        sciregressor_tester.test_configuration_loading()
    
    # =======================================
    # COMPREHENSIVE PYTEST FUNCTIONS (64 total)
    # =======================================
    
    # Phase 1: Individual Component Tests (48 functions)
    
    # 1.1 Hyperparameter Tuning Tests (12 functions)
    def test_xgb_hyperparameter_tuning_no_normalization(comprehensive_tester):
        """Test XGBoost hyperparameter tuning with no normalization."""
        assert comprehensive_tester.test_hyperparameter_tuning("xgb", "no_normalization")
    
    def test_xgb_hyperparameter_tuning_global_normalization(comprehensive_tester):
        """Test XGBoost hyperparameter tuning with global normalization."""
        assert comprehensive_tester.test_hyperparameter_tuning("xgb", "global_normalization")
    
    def test_xgb_hyperparameter_tuning_per_basin_normalization(comprehensive_tester):
        """Test XGBoost hyperparameter tuning with per-basin normalization."""
        assert comprehensive_tester.test_hyperparameter_tuning("xgb", "per_basin_normalization")
    
    def test_xgb_hyperparameter_tuning_long_term_mean_scaling(comprehensive_tester):
        """Test XGBoost hyperparameter tuning with long-term mean scaling."""
        assert comprehensive_tester.test_hyperparameter_tuning("xgb", "long_term_mean_scaling")
    
    def test_lgbm_hyperparameter_tuning_no_normalization(comprehensive_tester):
        """Test LightGBM hyperparameter tuning with no normalization."""
        assert comprehensive_tester.test_hyperparameter_tuning("lgbm", "no_normalization")
    
    def test_lgbm_hyperparameter_tuning_global_normalization(comprehensive_tester):
        """Test LightGBM hyperparameter tuning with global normalization."""
        assert comprehensive_tester.test_hyperparameter_tuning("lgbm", "global_normalization")
    
    def test_lgbm_hyperparameter_tuning_per_basin_normalization(comprehensive_tester):
        """Test LightGBM hyperparameter tuning with per-basin normalization."""
        assert comprehensive_tester.test_hyperparameter_tuning("lgbm", "per_basin_normalization")
    
    def test_lgbm_hyperparameter_tuning_long_term_mean_scaling(comprehensive_tester):
        """Test LightGBM hyperparameter tuning with long-term mean scaling."""
        assert comprehensive_tester.test_hyperparameter_tuning("lgbm", "long_term_mean_scaling")
    
    def test_catboost_hyperparameter_tuning_no_normalization(comprehensive_tester):
        """Test CatBoost hyperparameter tuning with no normalization."""
        assert comprehensive_tester.test_hyperparameter_tuning("catboost", "no_normalization")
    
    def test_catboost_hyperparameter_tuning_global_normalization(comprehensive_tester):
        """Test CatBoost hyperparameter tuning with global normalization."""
        assert comprehensive_tester.test_hyperparameter_tuning("catboost", "global_normalization")
    
    def test_catboost_hyperparameter_tuning_per_basin_normalization(comprehensive_tester):
        """Test CatBoost hyperparameter tuning with per-basin normalization."""
        assert comprehensive_tester.test_hyperparameter_tuning("catboost", "per_basin_normalization")
    
    def test_catboost_hyperparameter_tuning_long_term_mean_scaling(comprehensive_tester):
        """Test CatBoost hyperparameter tuning with long-term mean scaling."""
        assert comprehensive_tester.test_hyperparameter_tuning("catboost", "long_term_mean_scaling")
    
    # 1.2 Calibration Tests (12 functions)
    def test_xgb_calibration_no_normalization(comprehensive_tester):
        """Test XGBoost calibration with no normalization."""
        assert comprehensive_tester.test_calibration("xgb", "no_normalization")
    
    def test_xgb_calibration_global_normalization(comprehensive_tester):
        """Test XGBoost calibration with global normalization."""
        assert comprehensive_tester.test_calibration("xgb", "global_normalization")
    
    def test_xgb_calibration_per_basin_normalization(comprehensive_tester):
        """Test XGBoost calibration with per-basin normalization."""
        assert comprehensive_tester.test_calibration("xgb", "per_basin_normalization")
    
    def test_xgb_calibration_long_term_mean_scaling(comprehensive_tester):
        """Test XGBoost calibration with long-term mean scaling."""
        assert comprehensive_tester.test_calibration("xgb", "long_term_mean_scaling")
    
    def test_lgbm_calibration_no_normalization(comprehensive_tester):
        """Test LightGBM calibration with no normalization."""
        assert comprehensive_tester.test_calibration("lgbm", "no_normalization")
    
    def test_lgbm_calibration_global_normalization(comprehensive_tester):
        """Test LightGBM calibration with global normalization."""
        assert comprehensive_tester.test_calibration("lgbm", "global_normalization")
    
    def test_lgbm_calibration_per_basin_normalization(comprehensive_tester):
        """Test LightGBM calibration with per-basin normalization."""
        assert comprehensive_tester.test_calibration("lgbm", "per_basin_normalization")
    
    def test_lgbm_calibration_long_term_mean_scaling(comprehensive_tester):
        """Test LightGBM calibration with long-term mean scaling."""
        assert comprehensive_tester.test_calibration("lgbm", "long_term_mean_scaling")
    
    def test_catboost_calibration_no_normalization(comprehensive_tester):
        """Test CatBoost calibration with no normalization."""
        assert comprehensive_tester.test_calibration("catboost", "no_normalization")
    
    def test_catboost_calibration_global_normalization(comprehensive_tester):
        """Test CatBoost calibration with global normalization."""
        assert comprehensive_tester.test_calibration("catboost", "global_normalization")
    
    def test_catboost_calibration_per_basin_normalization(comprehensive_tester):
        """Test CatBoost calibration with per-basin normalization."""
        assert comprehensive_tester.test_calibration("catboost", "per_basin_normalization")
    
    def test_catboost_calibration_long_term_mean_scaling(comprehensive_tester):
        """Test CatBoost calibration with long-term mean scaling."""
        assert comprehensive_tester.test_calibration("catboost", "long_term_mean_scaling")
    
    # 1.3 Hindcast Tests (12 functions)
    def test_xgb_hindcast_no_normalization(comprehensive_tester):
        """Test XGBoost hindcast with no normalization."""
        assert comprehensive_tester.test_hindcast("xgb", "no_normalization")
    
    def test_xgb_hindcast_global_normalization(comprehensive_tester):
        """Test XGBoost hindcast with global normalization."""
        assert comprehensive_tester.test_hindcast("xgb", "global_normalization")
    
    def test_xgb_hindcast_per_basin_normalization(comprehensive_tester):
        """Test XGBoost hindcast with per-basin normalization."""
        assert comprehensive_tester.test_hindcast("xgb", "per_basin_normalization")
    
    def test_xgb_hindcast_long_term_mean_scaling(comprehensive_tester):
        """Test XGBoost hindcast with long-term mean scaling."""
        assert comprehensive_tester.test_hindcast("xgb", "long_term_mean_scaling")
    
    def test_lgbm_hindcast_no_normalization(comprehensive_tester):
        """Test LightGBM hindcast with no normalization."""
        assert comprehensive_tester.test_hindcast("lgbm", "no_normalization")
    
    def test_lgbm_hindcast_global_normalization(comprehensive_tester):
        """Test LightGBM hindcast with global normalization."""
        assert comprehensive_tester.test_hindcast("lgbm", "global_normalization")
    
    def test_lgbm_hindcast_per_basin_normalization(comprehensive_tester):
        """Test LightGBM hindcast with per-basin normalization."""
        assert comprehensive_tester.test_hindcast("lgbm", "per_basin_normalization")
    
    def test_lgbm_hindcast_long_term_mean_scaling(comprehensive_tester):
        """Test LightGBM hindcast with long-term mean scaling."""
        assert comprehensive_tester.test_hindcast("lgbm", "long_term_mean_scaling")
    
    def test_catboost_hindcast_no_normalization(comprehensive_tester):
        """Test CatBoost hindcast with no normalization."""
        assert comprehensive_tester.test_hindcast("catboost", "no_normalization")
    
    def test_catboost_hindcast_global_normalization(comprehensive_tester):
        """Test CatBoost hindcast with global normalization."""
        assert comprehensive_tester.test_hindcast("catboost", "global_normalization")
    
    def test_catboost_hindcast_per_basin_normalization(comprehensive_tester):
        """Test CatBoost hindcast with per-basin normalization."""
        assert comprehensive_tester.test_hindcast("catboost", "per_basin_normalization")
    
    def test_catboost_hindcast_long_term_mean_scaling(comprehensive_tester):
        """Test CatBoost hindcast with long-term mean scaling."""
        assert comprehensive_tester.test_hindcast("catboost", "long_term_mean_scaling")
    
    # 1.4 Operational Prediction Tests (12 functions)
    def test_xgb_operational_prediction_no_normalization(comprehensive_tester):
        """Test XGBoost operational prediction with no normalization."""
        assert comprehensive_tester.test_operational_prediction("xgb", "no_normalization")
    
    def test_xgb_operational_prediction_global_normalization(comprehensive_tester):
        """Test XGBoost operational prediction with global normalization."""
        assert comprehensive_tester.test_operational_prediction("xgb", "global_normalization")
    
    def test_xgb_operational_prediction_per_basin_normalization(comprehensive_tester):
        """Test XGBoost operational prediction with per-basin normalization."""
        assert comprehensive_tester.test_operational_prediction("xgb", "per_basin_normalization")
    
    def test_xgb_operational_prediction_long_term_mean_scaling(comprehensive_tester):
        """Test XGBoost operational prediction with long-term mean scaling."""
        assert comprehensive_tester.test_operational_prediction("xgb", "long_term_mean_scaling")
    
    def test_lgbm_operational_prediction_no_normalization(comprehensive_tester):
        """Test LightGBM operational prediction with no normalization."""
        assert comprehensive_tester.test_operational_prediction("lgbm", "no_normalization")
    
    def test_lgbm_operational_prediction_global_normalization(comprehensive_tester):
        """Test LightGBM operational prediction with global normalization."""
        assert comprehensive_tester.test_operational_prediction("lgbm", "global_normalization")
    
    def test_lgbm_operational_prediction_per_basin_normalization(comprehensive_tester):
        """Test LightGBM operational prediction with per-basin normalization."""
        assert comprehensive_tester.test_operational_prediction("lgbm", "per_basin_normalization")
    
    def test_lgbm_operational_prediction_long_term_mean_scaling(comprehensive_tester):
        """Test LightGBM operational prediction with long-term mean scaling."""
        assert comprehensive_tester.test_operational_prediction("lgbm", "long_term_mean_scaling")
    
    def test_catboost_operational_prediction_no_normalization(comprehensive_tester):
        """Test CatBoost operational prediction with no normalization."""
        assert comprehensive_tester.test_operational_prediction("catboost", "no_normalization")
    
    def test_catboost_operational_prediction_global_normalization(comprehensive_tester):
        """Test CatBoost operational prediction with global normalization."""
        assert comprehensive_tester.test_operational_prediction("catboost", "global_normalization")
    
    def test_catboost_operational_prediction_per_basin_normalization(comprehensive_tester):
        """Test CatBoost operational prediction with per-basin normalization."""
        assert comprehensive_tester.test_operational_prediction("catboost", "per_basin_normalization")
    
    def test_catboost_operational_prediction_long_term_mean_scaling(comprehensive_tester):
        """Test CatBoost operational prediction with long-term mean scaling."""
        assert comprehensive_tester.test_operational_prediction("catboost", "long_term_mean_scaling")
    
    # Phase 2: Complete Workflow Tests (12 functions)
    def test_xgb_complete_workflow_no_normalization(comprehensive_tester):
        """Test XGBoost complete workflow with no normalization."""
        assert comprehensive_tester.test_complete_workflow("xgb", "no_normalization")
    
    def test_xgb_complete_workflow_global_normalization(comprehensive_tester):
        """Test XGBoost complete workflow with global normalization."""
        assert comprehensive_tester.test_complete_workflow("xgb", "global_normalization")
    
    def test_xgb_complete_workflow_per_basin_normalization(comprehensive_tester):
        """Test XGBoost complete workflow with per-basin normalization."""
        assert comprehensive_tester.test_complete_workflow("xgb", "per_basin_normalization")
    
    def test_xgb_complete_workflow_long_term_mean_scaling(comprehensive_tester):
        """Test XGBoost complete workflow with long-term mean scaling."""
        assert comprehensive_tester.test_complete_workflow("xgb", "long_term_mean_scaling")
    
    def test_lgbm_complete_workflow_no_normalization(comprehensive_tester):
        """Test LightGBM complete workflow with no normalization."""
        assert comprehensive_tester.test_complete_workflow("lgbm", "no_normalization")
    
    def test_lgbm_complete_workflow_global_normalization(comprehensive_tester):
        """Test LightGBM complete workflow with global normalization."""
        assert comprehensive_tester.test_complete_workflow("lgbm", "global_normalization")
    
    def test_lgbm_complete_workflow_per_basin_normalization(comprehensive_tester):
        """Test LightGBM complete workflow with per-basin normalization."""
        assert comprehensive_tester.test_complete_workflow("lgbm", "per_basin_normalization")
    
    def test_lgbm_complete_workflow_long_term_mean_scaling(comprehensive_tester):
        """Test LightGBM complete workflow with long-term mean scaling."""
        assert comprehensive_tester.test_complete_workflow("lgbm", "long_term_mean_scaling")
    
    def test_catboost_complete_workflow_no_normalization(comprehensive_tester):
        """Test CatBoost complete workflow with no normalization."""
        assert comprehensive_tester.test_complete_workflow("catboost", "no_normalization")
    
    def test_catboost_complete_workflow_global_normalization(comprehensive_tester):
        """Test CatBoost complete workflow with global normalization."""
        assert comprehensive_tester.test_complete_workflow("catboost", "global_normalization")
    
    def test_catboost_complete_workflow_per_basin_normalization(comprehensive_tester):
        """Test CatBoost complete workflow with per-basin normalization."""
        assert comprehensive_tester.test_complete_workflow("catboost", "per_basin_normalization")
    
    def test_catboost_complete_workflow_long_term_mean_scaling(comprehensive_tester):
        """Test CatBoost complete workflow with long-term mean scaling."""
        assert comprehensive_tester.test_complete_workflow("catboost", "long_term_mean_scaling")
    
    # Phase 3: Cross-Model Integration Tests (4 functions)
    def test_multi_model_ensemble_no_normalization(comprehensive_tester):
        """Test multi-model ensemble with no normalization."""
        assert comprehensive_tester.test_multi_model_ensemble("no_normalization")
    
    def test_multi_model_ensemble_global_normalization(comprehensive_tester):
        """Test multi-model ensemble with global normalization."""
        assert comprehensive_tester.test_multi_model_ensemble("global_normalization")
    
    def test_multi_model_ensemble_per_basin_normalization(comprehensive_tester):
        """Test multi-model ensemble with per-basin normalization."""
        assert comprehensive_tester.test_multi_model_ensemble("per_basin_normalization")
    
    def test_multi_model_ensemble_long_term_mean_scaling(comprehensive_tester):
        """Test multi-model ensemble with long-term mean scaling."""
        assert comprehensive_tester.test_multi_model_ensemble("long_term_mean_scaling")
    
    # Comprehensive test runner
    def test_run_all_comprehensive_tests(comprehensive_tester):
        """Run all comprehensive tests in sequence."""
        assert comprehensive_tester.run_comprehensive_tests()
    

def main():
    """Main function for standalone execution."""
    import argparse
    
    parser = argparse.ArgumentParser(description='Run SciRegressor Model Tests')
    parser.add_argument('--verbose', '-v', action='store_true', 
                       help='Enable verbose logging (DEBUG level)')
    parser.add_argument('--log-file', type=str, default='test_sciregressor.log',
                       help='Log file name (default: test_sciregressor.log)')
    parser.add_argument('--only-failures', action='store_true',
                       help='Only show detailed logs for failed tests')
    parser.add_argument('--comprehensive', '-c', action='store_true',
                       help='Run comprehensive tests (64 total tests)')
    
    args = parser.parse_args()
    
    # Configure logging based on arguments
    log_level = logging.DEBUG if args.verbose else logging.INFO
    
    # Create custom formatter
    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    
    # Setup handlers
    handlers = []
    
    # Console handler
    console_handler = logging.StreamHandler()
    console_handler.setLevel(log_level)
    console_handler.setFormatter(formatter)
    handlers.append(console_handler)
    
    # File handler
    file_handler = logging.FileHandler(args.log_file, mode='w')
    file_handler.setLevel(logging.DEBUG)  # Always DEBUG in file
    file_handler.setFormatter(formatter)
    handlers.append(file_handler)
    
    # Configure root logger
    logging.basicConfig(level=logging.DEBUG, handlers=handlers, force=True)
    
    # Set specific logger levels
    if args.verbose:
        logging.getLogger('forecast_models.SciRegressor').setLevel(logging.DEBUG)
        logging.getLogger('scr.FeatureExtractor').setLevel(logging.DEBUG)
    else:
        logging.getLogger('forecast_models.SciRegressor').setLevel(logging.INFO)
        logging.getLogger('scr.FeatureExtractor').setLevel(logging.WARNING)
    
    print(f"Logging configured - Level: {log_level}, File: {args.log_file}")
    print(f"Verbose mode: {args.verbose}, Only failures: {args.only_failures}")
    print(f"Comprehensive mode: {args.comprehensive}")
    print("="*60)
    
    if args.comprehensive:
        # Run comprehensive tests
        tester = ComprehensiveSciRegressorTester()
        tester.only_failures = args.only_failures
        success = tester.run_comprehensive_tests()
    else:
        # Run original tests
        tester = SciRegressorTester()
        tester.only_failures = args.only_failures
        success = tester.run_all_tests()
    
    print("="*60)
    print(f"Check detailed logs in: {args.log_file}")
    
    return 0 if success else 1

if __name__ == "__main__":
    exit(main())
